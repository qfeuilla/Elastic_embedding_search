{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd080d14a88649dd46d429e21cbc1382463c00f035393205a877ff7ce1db51cc502",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "80d14a88649dd46d429e21cbc1382463c00f035393205a877ff7ce1db51cc502"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import string\n",
    "import re\n",
    "import io\n",
    "import ssl\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch.connection import create_ssl_context\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "open_distro_ssl_context = create_ssl_context()\n",
    "# next two lines are if you're running localhost with a self-signed cert (aka docker)\n",
    "open_distro_ssl_context.check_hostname = False\n",
    "open_distro_ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "es = Elasticsearch(\n",
    "    scheme=\"https\",\n",
    "    hosts=[ { 'port': 9200, 'host': 'localhost' } ],\n",
    "    ssl_context=open_distro_ssl_context,\n",
    "    http_auth=(\"admin\", \"admin\"), \n",
    "    timeout=30, \n",
    "    max_retries=10, \n",
    "    retry_on_timeout=True\n",
    ")\n",
    "\n",
    "# repalce the path with your json data file\n",
    "df = pd.read_json(\"./dblp-extrait/dblp-extrait.json\", lines=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "authors  removed\nn_citation  removed\nreferences  removed\nvenue  removed\nyear  removed\nid  removed\n"
     ]
    }
   ],
   "source": [
    "# this var should be the col you want to use for autocompletion\n",
    "col_to_keep = [\"abstract\", \"title\"]\n",
    "\n",
    "for c in df.columns:\n",
    "    if c not in col_to_keep:\n",
    "        print(c, \" removed\")\n",
    "        df = df.drop(columns=c)\n",
    "df.dropna(inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50662"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\ndef singularize(word):\\n    return word[:-1] if len(word) > 0 and word[-1] == 's' and (len(word) > 2 and word[-2] != 'i') else word\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "'''\n",
    "import inflect\n",
    "\n",
    "# used for semantic preprocessing\n",
    "p = inflect.engine()\n",
    "\n",
    "# kind of long maybe find another solution ?\n",
    "def singularize(word):\n",
    "    a = p.singular_noun(word)\n",
    "    return a if a is not False else word\n",
    "'''\n",
    "\n",
    "# faster but less semantic\n",
    "'''\n",
    "def singularize(word):\n",
    "    return word[:-1] if len(word) > 0 and word[-1] == 's' and (len(word) > 2 and word[-2] != 'i') else word\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use a list of common word by language (like the, for ...) and remove them from the string in this function\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    take a str and return a preprocessed str\n",
    "    current preprocessing are :\n",
    "    lowercase\n",
    "    punctuation removal\n",
    "    singularization -> not anymore too long and dont give good results\n",
    "    '''\n",
    "    words = text.split()\n",
    "    stripped =  [re.sub(r\"[,.;@#?!&$-]+\\ *\", \" \", w).lower() for w in words]\n",
    "    res = \" \".join(stripped)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 25331/25331 [00:10<00:00, 2303.13it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = []\n",
    "pbar = tqdm(total=df.size // 2)\n",
    "for item in df.iterrows():\n",
    "    item = item[1]\n",
    "    corpus.append((clean_text(item[1]), clean_text(item[0])))\n",
    "\n",
    "    pbar.update()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('preliminary design of a network protocol learning tool based on the comprehension of high school students: design by an empirical study using a simple mind map', 'the purpose of this study is to develop a learning tool for high school students studying the scientific aspects of information and communication net  works  more specifically  we focus on the basic principles of network proto  cols as the aim to develop our learning tool  our tool gives students hands on experience to help understand the basic principles of network protocols ')\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('loose particle classification using a new wavelet fisher discriminant method', 'loose particles left inside aerospace components or equipment can cause catastrophic failure in aerospace industry  it is vital to identify the material type of these loose particles and eliminate them  this is a classification problem  and autoregressive (ar) model and learning vector quantization (lvq) networks have been used to classify loose particles inside components  more recently  the test objects have been changed from components to aerospace equipments  to improve classification accuracy  more data samples often have to be dealt with  the difficulty is that these data samples contain redundant information  and the aforementioned two conventional methods are unable to process redundant information  thus the classification accuracy is deteriorated  in this paper  the wavelet fisher discriminant is investigated for loose particle classifications  first  the fisher model is formulated as a least squares problem with linear in the parameters structure  then  the previously proposed two stage subset selection method is used to build a sparse wavelet fisher model in order to reduce redundant information  experimental results show the wavelet fisher classification method can perform better than ar model and lvq networks ')\n"
     ]
    }
   ],
   "source": [
    "print(corpus[256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4/4 [00:14<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "utils = [\"embedd\", \"id2word\", \"word2id\"]\n",
    "ln_supported = [\"en\", \"de\", \"es\", \"fr\"]\n",
    "\n",
    "lang_utils = { ln: {utils[i]: d for i, d in enumerate(load_vec(f'wiki.multi.{ln}.vec'))} for ln in tqdm(ln_supported)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/qfeuilla/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = {\"en\": set(stopwords.words('english')), \"es\": set(stopwords.words('spanish')),\"de\": set(stopwords.words('german')),\"fr\": set(stopwords.words('french')),} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "PRETRAINED_MODEL_PATH = 'lid.176.bin'\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    return cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndef vectorize_v1(stop_words, ln_supported, lang_model, lang_utils, sent, perc_to_remove=0.1):\\n    embedds = []\\n    lang = lang_model.predict([sent])[0][0][0][-2:]\\n    if lang not in ln_supported:\\n        return np.zeros((600))\\n    word2id = lang_utils[lang][\"word2id\"]\\n    embedd = lang_utils[lang][\"embedd\"]\\n    words = sent.split()\\n    while len(words) and words[0] not in word2id:\\n        words = words[1:]\\n    if len(words) == 0:\\n        return np.zeros((600))\\n    ew = embedd[word2id[words[0]]]\\n    ew2 = None\\n    for i in range(1, len(words)):\\n        if words[i] not in stop_words[lang]:\\n            try:\\n                ew2 = embedd[word2id[words[i]]]\\n                embedds.append(np.concatenate([ew, ew2]))\\n                embedds.append(np.concatenate([ew2, ew]))\\n                ew = ew2\\n            except:\\n                None\\n    \\n    ew = embedd[word2id[words[0]]]\\n    if len(embedds) == 0:\\n        embedds.append(np.concatenate([ew, ew]))\\n    centroid = np.sum(embedds, axis=0) / len(embedds)\\n    return centroid\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "'''\n",
    "def vectorize_v1(stop_words, ln_supported, lang_model, lang_utils, sent, perc_to_remove=0.1):\n",
    "    embedds = []\n",
    "    lang = lang_model.predict([sent])[0][0][0][-2:]\n",
    "    if lang not in ln_supported:\n",
    "        return np.zeros((600))\n",
    "    word2id = lang_utils[lang][\"word2id\"]\n",
    "    embedd = lang_utils[lang][\"embedd\"]\n",
    "    words = sent.split()\n",
    "    while len(words) and words[0] not in word2id:\n",
    "        words = words[1:]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros((600))\n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    ew2 = None\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] not in stop_words[lang]:\n",
    "            try:\n",
    "                ew2 = embedd[word2id[words[i]]]\n",
    "                embedds.append(np.concatenate([ew, ew2]))\n",
    "                embedds.append(np.concatenate([ew2, ew]))\n",
    "                ew = ew2\n",
    "            except:\n",
    "                None\n",
    "    \n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    if len(embedds) == 0:\n",
    "        embedds.append(np.concatenate([ew, ew]))\n",
    "    centroid = np.sum(embedds, axis=0) / len(embedds)\n",
    "    return centroid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n# add outlier removal\\ndef vectorize_v2(stop_words, ln_supported, lang_model, lang_utils, sent, perc_to_remove=0.1):\\n    embedds = []\\n    lang = lang_model.predict([sent])[0][0][0][-2:]\\n    if lang not in ln_supported:\\n        return np.zeros((600))\\n    word2id = lang_utils[lang][\"word2id\"]\\n    embedd = lang_utils[lang][\"embedd\"]\\n    words = sent.split()\\n    while len(words) and words[0] not in word2id:\\n        words = words[1:]\\n    if len(words) == 0:\\n        return np.zeros((600))\\n    ew = embedd[word2id[words[0]]]\\n    ew2 = None\\n    for i in range(1, len(words)):\\n        if words[i] not in stop_words[lang]:\\n            try:\\n                ew2 = embedd[word2id[words[i]]]\\n                embedds.append(np.concatenate([ew, ew2]))\\n                embedds.append(np.concatenate([ew2, ew]))\\n                ew = ew2\\n            except:\\n                None\\n    \\n    ew = embedd[word2id[words[0]]]\\n    if len(embedds) == 0:\\n        embedds.append(np.concatenate([ew, ew]))\\n    centroid = np.sum(embedds, axis=0) / len(embedds) \\n    if len(embedds) * perc_to_remove >= 1:\\n        embedds = np.array(embedds)\\n        embedds = sorted(embedds.tolist(), key=lambda x: -np.array(dist([x], [centroid]))[0][0])\\n        centroid = np.sum(embedds[:-int(len(embedds) * perc_to_remove)], axis=0) / len(embedds) \\n    return centroid\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "'''\n",
    "# add outlier removal\n",
    "def vectorize_v2(stop_words, ln_supported, lang_model, lang_utils, sent, perc_to_remove=0.1):\n",
    "    embedds = []\n",
    "    lang = lang_model.predict([sent])[0][0][0][-2:]\n",
    "    if lang not in ln_supported:\n",
    "        return np.zeros((600))\n",
    "    word2id = lang_utils[lang][\"word2id\"]\n",
    "    embedd = lang_utils[lang][\"embedd\"]\n",
    "    words = sent.split()\n",
    "    while len(words) and words[0] not in word2id:\n",
    "        words = words[1:]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros((600))\n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    ew2 = None\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] not in stop_words[lang]:\n",
    "            try:\n",
    "                ew2 = embedd[word2id[words[i]]]\n",
    "                embedds.append(np.concatenate([ew, ew2]))\n",
    "                embedds.append(np.concatenate([ew2, ew]))\n",
    "                ew = ew2\n",
    "            except:\n",
    "                None\n",
    "    \n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    if len(embedds) == 0:\n",
    "        embedds.append(np.concatenate([ew, ew]))\n",
    "    centroid = np.sum(embedds, axis=0) / len(embedds) \n",
    "    if len(embedds) * perc_to_remove >= 1:\n",
    "        embedds = np.array(embedds)\n",
    "        embedds = sorted(embedds.tolist(), key=lambda x: -np.array(dist([x], [centroid]))[0][0])\n",
    "        centroid = np.sum(embedds[:-int(len(embedds) * perc_to_remove)], axis=0) / len(embedds) \n",
    "    return centroid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\na = vectorize_v2(stop_words, ln_supported, model, lang_utils, \"loose particles left inside aerospace components or equipment can cause catastrophic failure in aerospace industry  it is vital to identify the material type of these loose particles and eliminate them  this is a classification problem  and autoregressive (ar) model and learning vector quantization (lvq) networks have been used to classify loose particles inside components  more recently  the test objects have been changed from components to aerospace equipments  to improve classification accuracy  more data samples often have to be dealt with  the difficulty is that these data samples contain redundant information  and the aforementioned two conventional methods are unable to process redundant information  thus the classification accuracy is deteriorated  in this paper  the wavelet fisher discriminant is investigated for loose particle classifications  first  the fisher model is formulated as a least squares problem with linear in the parameters structure  then  the previously proposed two stage subset selection method is used to build a sparse wavelet fisher model in order to reduce redundant information  experimental results show the wavelet fisher classification method can perform better than ar model and lvq networks\")\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "'''\n",
    "a = vectorize_v2(stop_words, ln_supported, model, lang_utils, \"loose particles left inside aerospace components or equipment can cause catastrophic failure in aerospace industry  it is vital to identify the material type of these loose particles and eliminate them  this is a classification problem  and autoregressive (ar) model and learning vector quantization (lvq) networks have been used to classify loose particles inside components  more recently  the test objects have been changed from components to aerospace equipments  to improve classification accuracy  more data samples often have to be dealt with  the difficulty is that these data samples contain redundant information  and the aforementioned two conventional methods are unable to process redundant information  thus the classification accuracy is deteriorated  in this paper  the wavelet fisher discriminant is investigated for loose particle classifications  first  the fisher model is formulated as a least squares problem with linear in the parameters structure  then  the previously proposed two stage subset selection method is used to build a sparse wavelet fisher model in order to reduce redundant information  experimental results show the wavelet fisher classification method can perform better than ar model and lvq networks\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accelerate v2\n",
    "def vectorize_v3(stop_words, ln_supported, lang_model, lang_utils, sent, perc_to_remove=0.1):\n",
    "    embedds = []\n",
    "    lang = lang_model.predict([sent])[0][0][0][-2:]\n",
    "    if lang not in ln_supported:\n",
    "        return np.zeros((600))\n",
    "    word2id = lang_utils[lang][\"word2id\"]\n",
    "    embedd = lang_utils[lang][\"embedd\"]\n",
    "    words = sent.split()\n",
    "    while len(words) and words[0] not in word2id:\n",
    "        words = words[1:]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros((600))\n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    ew2 = None\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] not in stop_words[lang]:\n",
    "            try:\n",
    "                ew2 = embedd[word2id[words[i]]]\n",
    "                embedds.append(np.concatenate([ew, ew2]))\n",
    "                embedds.append(np.concatenate([ew2, ew]))\n",
    "                ew = ew2\n",
    "            except:\n",
    "                None\n",
    "    \n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    if len(embedds) == 0:\n",
    "        embedds.append(np.concatenate([ew, ew]))\n",
    "    centroid = np.sum(embedds, axis=0) / len(embedds) \n",
    "    if len(embedds) * perc_to_remove >= 1:\n",
    "        embedds = np.array(embedds)\n",
    "        sims = np.squeeze(dist(embedds, [centroid]))\n",
    "        zipped_lists = np.array(sorted(zip(embedds, sims), key=lambda x:x[1]), dtype=np.object)\n",
    "        embedds = zipped_lists[:, 0]\n",
    "        centroid = np.sum(embedds[:-int(len(embedds) * perc_to_remove)], axis=0) / len(embedds) \n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a = vectorize_v3(stop_words, ln_supported, model, lang_utils, \"loose particles left inside aerospace components or equipment can cause catastrophic failure in aerospace industry  it is vital to identify the material type of these loose particles and eliminate them  this is a classification problem  and autoregressive (ar) model and learning vector quantization (lvq) networks have been used to classify loose particles inside components  more recently  the test objects have been changed from components to aerospace equipments  to improve classification accuracy  more data samples often have to be dealt with  the difficulty is that these data samples contain redundant information  and the aforementioned two conventional methods are unable to process redundant information  thus the classification accuracy is deteriorated  in this paper  the wavelet fisher discriminant is investigated for loose particle classifications  first  the fisher model is formulated as a least squares problem with linear in the parameters structure  then  the previously proposed two stage subset selection method is used to build a sparse wavelet fisher model in order to reduce redundant information  experimental results show the wavelet fisher classification method can perform better than ar model and lvq networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(stop_words, ln_supported, lang_model, lang_utils, s1, s2, prt=True):\n",
    "    sim = dist([vectorize_v3(stop_words, ln_supported, lang_model, lang_utils, s1)], [vectorize_v3(stop_words, ln_supported, lang_model, lang_utils, s2)])[0]\n",
    "    if prt:\n",
    "        print(f\"similarity between \\\"{s1}\\\" and \\\"{s2}\\\" : {sim}\")\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "similarity between \"the purpose of this study is to develop a learning tool for high school student studying the scientific aspect of information and communication network\" and \"scientific\" : [0.53081813]\nsimilarity between \"introduction public health surveillance system need to be refined\" and \"scientific\" : [0.4004291]\n"
     ]
    }
   ],
   "source": [
    "s2 = similarity(stop_words, ln_supported, model, lang_utils, \"the purpose of this study is to develop a learning tool for high school student studying the scientific aspect of information and communication network\", \"scientific\")\n",
    "s3 = similarity(stop_words, ln_supported, model, lang_utils, \"introduction public health surveillance system need to be refined\", \"scientific\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_generator(corpus):\n",
    "    for line in tqdm(corpus):\n",
    "        yield {\n",
    "            \"_index\" : 'toy_data_docs_embb',\n",
    "            \"_source\" : {\n",
    "                \"title\": line[0],\n",
    "                \"data\" : line[1],\n",
    "                \"title_embeddings\": vectorize_v3(stop_words, ln_supported, model, lang_utils, line[0]),\n",
    "                \"data_embeddings\": vectorize_v3(stop_words, ln_supported, model, lang_utils, line[1])\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "source": [
    "before executing next cell execute in kibana :\n",
    "```\n",
    "\n",
    "DELETE /toy_data_docs_embb\n",
    "\n",
    "PUT /toy_data_docs_embb\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"knn\": true,\n",
    "      \"knn.space_type\": \"cosinesimil\"\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"title\": { \n",
    "        \"type\" : \"text\"\n",
    "      },\n",
    "      \"data\": { \n",
    "        \"type\" : \"text\"\n",
    "      },\n",
    "      \"title_embeddings\": {\n",
    "        \"type\": \"knn_vector\", \n",
    "        \"dimension\": 600\n",
    "      },\n",
    "      \"data_embeddings\": {\n",
    "        \"type\": \"knn_vector\", \n",
    "        \"dimension\": 600\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  2%|▏         | 492/25331 [00:02<02:09, 191.97it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "  4%|▍         | 998/25331 [00:06<01:59, 203.48it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "  6%|▌         | 1482/25331 [00:10<02:03, 193.55it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "  8%|▊         | 1985/25331 [00:15<02:04, 188.04it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 10%|▉         | 2487/25331 [00:19<02:03, 185.17it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 12%|█▏        | 2988/25331 [00:23<02:18, 161.80it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 14%|█▍        | 3489/25331 [00:27<01:55, 188.56it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 16%|█▌        | 3979/25331 [00:31<01:50, 194.01it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 18%|█▊        | 4483/25331 [00:35<01:44, 199.15it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 20%|█▉        | 4996/25331 [00:39<02:05, 162.60it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 22%|██▏       | 5496/25331 [00:44<01:35, 207.80it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 24%|██▎       | 5984/25331 [00:47<01:39, 194.65it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 26%|██▌       | 6490/25331 [00:52<01:38, 190.53it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 28%|██▊       | 6988/25331 [00:55<01:32, 198.99it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 30%|██▉       | 7499/25331 [00:59<01:39, 180.02it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 32%|███▏      | 7984/25331 [01:03<01:27, 198.52it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 33%|███▎      | 8482/25331 [01:07<01:24, 200.09it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 35%|███▌      | 8988/25331 [01:11<01:26, 190.01it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 37%|███▋      | 9498/25331 [01:15<01:24, 186.51it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 39%|███▉      | 9995/25331 [01:19<01:20, 191.45it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 41%|████▏     | 10493/25331 [01:23<01:20, 183.49it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 43%|████▎     | 10981/25331 [01:27<01:23, 172.34it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 45%|████▌     | 11493/25331 [01:31<01:15, 182.97it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 47%|████▋     | 11992/25331 [01:35<01:18, 170.77it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 49%|████▉     | 12495/25331 [01:39<01:04, 198.83it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 51%|█████▏    | 12991/25331 [01:43<01:03, 193.58it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 53%|█████▎    | 13481/25331 [01:47<01:01, 191.62it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 55%|█████▌    | 13986/25331 [01:51<01:06, 170.21it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 57%|█████▋    | 14500/25331 [01:56<01:00, 179.01it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 59%|█████▉    | 14990/25331 [02:00<00:52, 195.61it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 61%|██████    | 15487/25331 [02:05<01:03, 154.37it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 63%|██████▎   | 15995/25331 [02:09<00:52, 177.79it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 65%|██████▌   | 16488/25331 [02:13<00:47, 186.91it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 67%|██████▋   | 17000/25331 [02:17<00:44, 188.67it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 69%|██████▉   | 17494/25331 [02:21<00:42, 183.50it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 71%|███████   | 17997/25331 [02:26<00:39, 187.30it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 73%|███████▎  | 18499/25331 [02:30<00:38, 176.06it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 75%|███████▍  | 18990/25331 [02:34<00:33, 187.11it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 77%|███████▋  | 19498/25331 [02:51<00:31, 184.71it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 79%|███████▉  | 19993/25331 [03:13<00:44, 119.84it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 81%|████████  | 20483/25331 [03:18<00:26, 183.16it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 83%|████████▎ | 20984/25331 [03:23<00:25, 170.14it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 85%|████████▍ | 21492/25331 [03:28<00:22, 171.61it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 87%|████████▋ | 21994/25331 [03:34<00:27, 119.24it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 89%|████████▉ | 22482/25331 [03:39<00:16, 169.79it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 91%|█████████ | 22990/25331 [03:43<00:13, 167.87it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 93%|█████████▎| 23486/25331 [03:48<00:13, 138.48it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 95%|█████████▍| 24000/25331 [03:52<00:07, 168.71it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 97%|█████████▋| 24493/25331 [03:57<00:04, 176.04it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      " 99%|█████████▊| 24991/25331 [04:01<00:02, 161.69it/s]/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "100%|██████████| 25331/25331 [04:05<00:00, 103.35it/s]\n",
      "/home/qfeuilla/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "out = helpers.bulk(es, doc_generator(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "understanding election candidate approval ratings using social media data; the last few years has seen an exponential increase in the amount of social media data generated daily  thus  researchers have started exploring the use of social media data in building recommendation systems  prediction models  improving disaster management  discovery trending topics etc  an interesting application of social media is for the prediction of election results  the recently conducted 2012 us presidential election was the \"most tweeted\" election in history and provides a rich source of social media posts  previous work on predicting election outcomes from social media has been largely been based on sentiment about candidates  total volumes of tweets expressing electoral polarity and the like  in this paper we use a collection of tweets to predict the daily approval ratings of the two us presidential candidates and also identify topics that were causal to the approval ratings \n\nthe significance of bidding  accepting and opponent modeling in automated negotiation; given the growing interest in automated negotiation  the search for effective strategies has produced a variety of different negotiation agents  despite their diversity  there is a common structure to their design  a negotiation agent comprises three key components: the bidding strategy  the opponent model and the acceptance criteria  we show that this three component view of a negotiating architecture not only provides a useful basis for developing such agents but also provides a useful analytical tool  by combining these components in varying ways  we are able to demonstrate the contribution of each component to the overall negotiation result  and thus determine the key contributing components  moreover  we study the interaction between components and present detailed interaction effects  furthermore  we find that the bidding strategy in particular is of critical importance to the negotiator's success and far exceeds the importance of opponent preference modeling techniques  our results contribute to the shaping of a research agenda for negotiating agent design by providing guidelines on how agent developers can spend their time most effectively \n\nopponent modelling in texas hold'em poker as the key for success; over the last few years  research in artificial intelligence has focussed on games with incomplete information and non deterministic moves  the game of poker is a perfect theme for studying this subject  the best known poker variant is texas hold'em that combines simple rules with a huge amount of possible playing strategies  this paper is focussed on developing algorithms for performing simple online opponent modelling in texas hold'em poker enabling to select the best strategy to play against each given opponent  several autonomous agents were developed in order to simulate typical poker player's behaviour and an observer agent was developed  capable of using simple opponent modelling techniques  in order to select the best playing strategy against each opponent  the results obtained in realistic experiments using eight distinct poker playing agents showed the usefulness of the approach  the observer agent is clearly capable of outperforming all their counterparts in all tests performed \n\nthe influence of social networking sites on participation in the 2012 presidential election; social networking sites are gaining in popularity  and candidates for president have been getting more involved in these online platforms  in order to examine whether a presidential candidate's presence on social networking sites influences people's political participation  we conducted a survey asking users a series of questions related to their social networking involvement  political involvement  and political involvement on social networking sites  specifically with regard to the 2012 presidential election  our results indicate that despite being politically minded  these users do not use facebook for political reasons and a candidate's online presence does not influence their decision on how to vote \n\nquantifying political leaning from tweets and retweets; media outlets and pundits have been quick to embrace online social networks to disseminate their own opinions  but pundits’ opinions and news coverage are often marked by a clear political bias  as widely evidenced during the fiercely contested 2012 u s  presidential elections  given the wide availability of such data from sites like twitter  a natural question is whether we can quantify the political leanings of media outlets using osn data  in this work  by drawing a correspondence between tweeting and retweeting behavior  we formulate political leaning estimation as an ill posed linear inverse problem  the result is a simple and scalable approach that does not require explicit knowledge of the network topology  we evaluate our method with a dataset of 119 million election related tweets collected from april to november  and use it to study the political leaning of prominent tweeters and media sources \n\npower dynamics in spoken interactions: a case study on 2012 republican primary debates; in this paper  we explore how the power differential between participants of an interaction affects the way they interact in the context of political debates  we analyze the 2012 republican presidential primary debates where we model the power index of each candidate in terms of their poll standings  we find that the candidates' power indices affected the way they interacted with others in the debates as well as how others interacted with them \n\na formal security model of a smart card web server; smart card web server provides a modern interface between smart cards and the external world  it is of paramount importance that this new software component does not jeopardize the security of the smart card  this paper presents a formal model of the smart card web server specification and the proof of its security properties  the formalization enables a thoughtful analysis of the specification that has revealed several ambiguities and potentially dangerous behaviors  our formal model is built using a modular approach upon a model of java card and global platform  by proving the security properties  we show that the smart card web server preserves the security policy of the overall model  in other words  this component introduces no illegal access to the card resources ( i e  file system and applications)  furthermore  the smart card web server provides a means for securely managing the card contents ( i e  resources update) \n\nproposing candidate views for materialization; view selection concerns selection of appropriate set of views for ma  terialization subject to constraints like size  space  time etc  however  selecting optimal set of views for a higher dimensional data set is an np hard problem  alternatively  views can be selected by exploring the search space in a greedy manner  several greedy algorithms for view selection exist in literature among which hrua is considered the most fundamental  hrua exhibits high run time complexity primarily because the number of possible views that it needs to evaluate is exponential in the number of dimensions  as a result  it would be  come infeasible to select views for higher dimensional data sets  the proposed views greedy algorithm (pvga)  presented in this paper  addresses this prob  lem by selecting views from a smaller set of proposed views  instead of all the views in the lattice as in case of hrua  this would make view selection more efficient and feasible for higher dimensional data  further  it was experimen  tally found that pvga trades significant improvement in time to evaluate all views with a slight drop in the quality of views selected for materialization \n\nintonation in political speech: ségolène royal vs  nicolas sarkozy; this study is not about the text or the rhetorical properties of the political discourse  but rather about the way the segments of text are assembled prosodically and delivered  the analysis of two short but typical examples pertaining to two former candidates to the french presidential elections held in 2007  i e  segolene royal sr and nicolas sarkozy ns  reveals that these leading political leaders differ in similar environment 1 in the complexity of the delivered prosodic structures  2 in the congruence or non congruence of prosody with syntax and 3 in the realization of continuation melodic contours  although one would expect a style more formal for ns and more relaxed for sr considering their respective political opinions ns close to the right  sr to the left  the prosodic style used by ns appears to be closer to the spontaneous speech used in non formal conditions  whereas sr consistently uses a very formal and conservative organization of her public speeches \n\nbidflow: a new graph based bidding language for combinatorial auctions; in this paper we introduce a new graph based bidding language for combinatorial auctions  in our language  each bidder submits to the arbitrator a generalized flow network (netbid) representing her bids  the interpretation of the winner determination problem as an aggregation of individual preferences represented as flowbids allows building an aggregate netbid for its representation  labelling the nodes with appropriate procedural functions considerably improves upon the expressivity of previous bidding languages \n\nmassive media event data analysis to assess world wide political conflict and instability; mining massive daily news media data to infer patterns of cultural trends  including political conflicts and instabilities  is an important goal of computational social science and the new interdisciplinary field called \"culturnomics \" while the sheer size of media data makes this task challenging  a greater hurdle is the nonstationarity of data  manifested in several ways  which invalidates surge in media coverage as a reliable indicator of political change  we demonstrate the use of advanced statistical  information theoretic  and random fractal methods to analyze cameo encoded political events data  in particular  we show that on the country level  event distributions obey a zipf mandelbrot law  and interactions among countries follow an exponential law  indicating that local or prioritized events dominate the political environment of a country  most importantly  we find that world wide political instabilities  such as the arab spring  are associated with breakdown or enhancement of long range correlations in political events \n\nconvergence analysis for weighted joint strategy fictitious play in generalized second price auction; generalized second price (gsp) auction is one of the most commonly used auction mechanisms in sponsored search  as compared to conventional equilibrium analyses on gsp auctions  the convergence analysis on the dynamic behaviors of the bidders can better describe real world sponsored search systems  and give them a more useful guideline for making improvement  however  most existing works on convergence analysis assume the bidders to be greedy in taking actions  i e  they only utilize the bid information in the current round of auction when determining the best strategy for the next round  we argue that real world professional advertisers are more capable and can utilize the information in a longer history to optimize their strategies  accordingly  we propose modeling their behaviors by a weighted joint strategy fictitious play (wjsfp)  in the proposed model  bidders determine their optimal strategies based on their beliefs on other bidders' bid prices  and the beliefs are updated by considering all the information they have received so far in an iterative manner  we have obtained the following theoretical results regarding the proposed model: 1) when there are only two ad slots  the bid profile of the bidders will definitely converge  when there are multiple slots  there is a sufficient condition that guarantees the convergence of the bid profile  2) as long as the bid profile can converge  it converges to a nash equilibrium of gsp  to the best of our knowledge  this is the first time that the joint strategy fictitious play is adopted in such a complex game as sponsored search auctions \n\nidentification of the minimal set of attributes that maximizes the information towards the author of a political discourse: the case of the candidates in the mexican presidential elections; authorship attribution has attracted the attention of the natural language processing and machine learning communities in the past few years  here we are interested in finding a general measure of the style followed in the texts from the three main candidates in the mexican presidential elections of 2012  we analyzed dozens of texts (discourses) from the three authors  we applied tools from the time series processing field and machine learning community in order to identify the overall attributes that define the writing style of the three authors  several attributes and time series were extracted from each text  a novel methodology  based in mutual information  was applied on those time series and attributes to explore the relevance of each attribute to linearly separate the texts accordingly to their authorship  we show that less than 20 variables are enough to identify  by means of a linear recognizer  the authorship of a text from within one of the three considered authors \n\ndefining privacy for weighted votes  single and multi voter coercion; most existing formal privacy definitions for voting protocols are based on observational equivalence between two situations where two voters swap their votes  these definitions are unsuitable for cases where votes are weighted  in such a case swapping two votes can result in a different outcome and both situations become trivially distinguishable  we present a definition for privacy in voting protocols in the applied pi calculus that addresses this problem  using our model  we are also able to define multi voter coercion  i e  situations where several voters are attacked at the same time  then we prove that under certain realistic assumptions a protocol secure against coercion of a single voter is also secure against coercion of multiple voters  this applies for receipt freeness as well as coercion resistance \n\nthreshold start up control policy for polling systems; a threshold start up policy is appealing for manufacturing (service) facilities that incur a cost for keeping the machine (server) on  as well as for each restart of the server from its dormant state  analysis of single product (customer) systems operating under such a policy  also known as the n policy  has been available for some time  this article develops mathematical analysis for multiproduct systems operating under a cyclic exhaustive or globally gated service regime and a threshold start up rule  it pays particular attention to modeling switchover (setup) times  the analysis extends/unifies existing literature on polling models by obtaining as special cases  the continuously roving server and patient server polling models on the one hand  and the standard m/g/1 queue with n policy  on the other hand  we provide a computationally efficient algorithm for finding aggregate performance measures  such as the mean waiting time for each customer type and the mean unfinished work in system  we show that the search for the optimal threshold level can be restricted to a finite set of possibilities \n"
     ]
    }
   ],
   "source": [
    "# full vector search\n",
    "size = 15\n",
    "\n",
    "query = clean_text(\"trump\")\n",
    "query_vec = vectorize_v3(stop_words, ln_supported, model, lang_utils, query)\n",
    "\n",
    "query_body = {\n",
    "  \"size\": size,\n",
    "  \"query\": {\n",
    "    \"script_score\": {\n",
    "      \"query\": {\n",
    "        \"match_all\": { }\n",
    "      },\n",
    "      \"script\": {\n",
    "        \"source\": \"cosineSimilarity(params.query_value, doc[params.field1]) + cosineSimilarity(params.query_value, doc[params.field2])\",\n",
    "        \"params\": {\n",
    "          \"field1\": \"title_embeddings\",\n",
    "          \"field2\": \"data_embeddings\",\n",
    "          \"query_value\": query_vec\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "res = es.search(index=\"toy_data_docs_embb\", body=query_body)\n",
    "print('\\n\\n'.join(['; '.join([res['hits']['hits'][i]['_source']['title'] + '', res['hits']['hits'][i]['_source']['data']]) for i in range(len(res['hits']['hits']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2965\n"
     ]
    }
   ],
   "source": [
    "print(res['took'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "phishing attacks detection using genetic programming; phishing is a real threat on the internet nowadays  according to a re  port released by an american security firm  rsa  there have been approximately 33 000 phishing attacks globally each month in 2012  leading to a loss of  687 mil  lion  therefore  fighting against phishing attacks is of great importance  one popu  lar and widely deployed solution with browsers is to integrate a blacklist sites into them  however  this solution  which is unable to detect new attacks if the database is out of date  appears to be not effective when there are a lager number of phish  ing attacks created very day  in this paper  we propose a solution to this problem by applying genetic programming to phishing detection problem  we conducted the experiments on a data set including both phishing and legitimate sites collected from the internet  we compared the performance of genetic programming with a number of other machine learning techniques and the results showed that genetic programming produced the best solutions to phishing detection problem ;         score 3.9669662\n\ncomponent labeling on simd spmd architecture; v ~~(~s( nt a conlponent 1al)elirrg algorithm for a coarse grained i) i~ iiii>i  ~rcl~itcctnrc~ rr 1111ctl opsii \\  opsil:\\ ih an rsperin~rr~  i ii g(>n(~r ri 1)11r1)ow n111ltil)rocessor \\virici~ nses t\\vo tlifferent for~ns of ~~~ri~llrlisnr  'i'l~r first  on(  is a well known si \\ii) mode (s~ii  ~ii~~)iioiis ~~~otlr) and rlrr sccootl onr is called si' \\iil (single i'ro  :~ iiii s111itiple 1)iita stroa~n) \\viriclr is a11 asy~~cliro~ro~~s iiio~~  \\yp ilavi  sl~o\\vn oi'sill\\'s rffict ivcnrss for the low level i~nagr pro  voshing (i)ii~rh~i) and \\vo d(~111011st~ri1te irere its effectiveness for t iip i~~le~r~n(~(lii~ to l(>vel ~vitli a co1111)one11t 1al)eli11g i~~~~)le~i~e~rla tio~l  i'lrr ainr of tile inlplenle~rtation is ol~tained after processi~rg the i)c x~ loatl 1)alanring efficiency  so  \\ve propose a loatl tlistribu  lion illgoritl~nl \\vliic11 i1icor1)orates a tolerance factor on the load fl~nclio~l  'l'l~is algoritl~n~ call take illto consideration tlrc initial lo  c ali~i~tion of region ant1 tve slioiv it is the better choice lo the loatl i)ali~~~ciug clficiency vcrst~s tlrc cost  of list tranhfer  some res~~lts 4y1111)o1s (1)ix~i ~~~irt rix 3 list set)  'l'l~(' typical aigoritl1111 of the int(~rl~~~~(li ~ t(~ i t v( l is tl~t  co~npon(~nt lal)eli~~g algoritlr~n (13aix2)   ~ii(ii(yi 111~ ~~i~r ~li(~lis~~ r of conipo~rent 1ai)rling (sl1ix2)  (hiiii~~~)  (1)11fl's(i)  (('yi)\" ))  (i iiii)x )) 011 hilhll) or fine grain si \\td arclii  11'1 t11ri'h  11111 coiii~)oii~ii~ 1al)c~lilrg is j11s1   i stol) lo\\vards tlrr final  ( i 11i  i~lt~~rl~~~(~t r tio~~ 1))  tl~i> 1)itl~ 'l' (rtwi~rrl~ ii~~ii~~~iii~~ii ~ of frc~rcl~ :i~iii~)  ol'sil:\\ i i ifrt or o arallclis~~i lie 's (sie) tlre lirst i)ii(  is  i \\vrll kllo\\yll stmi1 lnotlc (s~iic~ii ~iioii ~ nrotle) ant1 111o ~~~c ontl one is called si'lid (single 1'ropl i\\111 sll~ltiple data +trcir~~~) \\\\ l~ivll ih ~iii asy11chro11011h nrotle   \\ian   ~l)plications irave i)c5c>11 i~ul)lr~~~c~rt c ~tl o~r oi'sila lilic finite ele~uc~~ts (sl~nxi)  ray 11 tcillg algoritlr~n (forsx) or particule accc lerator (ilaips)  for tlre ( 'o~~il)~~trr \\ ision  \\ve )rave siroi~~11 opsil:\\'s effc>ctiveness for the lo\\v ii~v~~l i~n rg(  procc>ssi~~g (1)11chsa)  111(l \\v(  ~ic~~~rc)~rhtr rt(  lrrrr its isf(i~l i\\ rnrss for ill(  int rr~nctlii~te love1 witli a co~nponc~n t lal~el  illy ~iii~)~(~iii(~ii ~~~~oii  '1'11(  ail11 of t111t i~nplrnrr~rtatio n oi)tainrtl after processi~~g tlre i)est loacl balancing efficiency  if wt  hay t11(  processor load tlepe~~tls on the nu~nber of region pisrls  we want ilftrr processing tlie same ~rnrnber of pixels irr each prorrssor  111 gen(>ral  this involves that \\ve woii'~ get the salnc iiiiiii~)~~ of rrgio~r in eacl~ processor ant1 therc~forr involves a commnnication rost he  t~vc~c~~~ processors wliicl~ must be taken into account  so  ivc 1)roposr a load tlistribution algoritl~rn \\vl~icl~ incorl)o  ratcs a tolerance factor on tlre load function  tlris algoritl~~n can take into consitleration t11(  initial localization of rrgion i~ntl wr s11o\\v it is the i)rtt er cl~oicc to tlrr loatl i)ala~rci~~g efficiency vrrsll:  tlre cost of list transfer  sorne examples and results are given on reill inrage  2 presentation of opsila oi'sila is a general purpose parallel arclritecturr belo~rging to tlrc ~ilised cla ss (see (i\\ugsi) for inore tlrtails)  it rnns \\vitli two tlilfcrent forlrrx or ~)i~rirlleli~~~i   the first one is the \\vcll  ~iio\\vii slhld l~rotle (a syncl~ronot~s form of parallelism) (reexl)  tlre see  ontl one is callrtl sphld (single progran~  si~~ltiplc data strea111) wlriclr is an asy~~cl~ro~rous triode  tllesc tv 0 ~notlcs are tly1ia111  ically configurable in one ~nachinc cycle  opsila is colnposctl of t wo pa rts :  i centr ~l cont ol nnit and a parallel coirrl)~~tation tunit  ~vlricli cont ains p = ici processors  1) mcniory nnits i~nd an omega /benes ir~terco~lnection nct~vork (fenghl)  111 silid ~notlc  illc~mory is ~lli1i'otl : (yrcll i)roressor cilll arc1~ss ill illly ilddl'~~~ of inelnory  in sl'hld ~~rotlr  c>i~ch procrshor acccssrs only at its lorill nren1ory  tiif scalar ccjiii rol 1111it perfor1115 ov( rall n~anirgenient of t iiv system  it consists in two processors : the scalar i'rocessor (sl') ant1 the ilrstrnct ion i'roccssor (ip)  'l'l~r ip inanages thr vrctor unit (menrory  ~~c twork  vrct or instrnctions and sy~rcl~ro~~isi~t io~~s i)et ween simd and si'bld ~nodes)  the vector unit  consists of p=2\"= l (i ele~nentary l'roccssors ( pes)  each associatctl wit11 a iiiciiio~~ l)i\\~lk ( \\in)  t\\ sy~~eirron(~ omega/nenes interco~rncctio~ ~ nc t\\vork is rlsrtl to perfor~n data cxchangcs  particnlar facilities are providetl as any length vec  tors  al~tomatic inerrlory ant1 nc~t\\vo~ li iii~iii~~~~iiici;         score 1.2324501\n"
     ]
    }
   ],
   "source": [
    "# hybrid v1\n",
    "size = 15\n",
    "\n",
    "query = clean_text(\"lion\")\n",
    "query_vec = vectorize_v3(stop_words, ln_supported, model, lang_utils, query)\n",
    "\n",
    "query_body = {\n",
    "  \"size\": size,\n",
    "  \"query\": {\n",
    "    \"function_score\": {\n",
    "      \"query\": {\n",
    "        \"multi_match\": { \n",
    "          \"query\": query,\n",
    "          \"fields\": [\"data\", \"title\"]\n",
    "        }\n",
    "      },\n",
    "      \"script_score\": {\n",
    "        \"script\": {\n",
    "          \"source\": \"cosineSimilarity(params.query_value, doc[params.field1]) + cosineSimilarity(params.query_value, doc[params.field2])\",\n",
    "          \"params\": {\n",
    "            \"field1\": \"title_embeddings\",\n",
    "            \"field2\": \"data_embeddings\",\n",
    "            \"query_value\": query_vec\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "res = es.search(index=\"toy_data_docs_embb\", body=query_body)\n",
    "print('\\n\\n'.join(['; '.join([res['hits']['hits'][i]['_source']['title'], res['hits']['hits'][i]['_source']['data'], f\"        score {res['hits']['hits'][i]['_score']}\"]) for i in range(len(res['hits']['hits']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a knowledge framework for natural language analysis; recent research in language analysis and language generation has highlighted the role of knowledge representation in both processes  certain knowledge representation foundations  such as structured inheritance networks and feature based linguistic representations  have proved useful in a variety of language processing tasks  augmentations to this common framework  however  are required to handle particular issues  such as the role relationship problem: the task of determining how roles  or slots  of a given frame  are filled based on knowledge about other roles  three knowledge structures are discussed that address this problem  the semantic interpreter of an analyzer called trump (transportable understanding mechanism package) uses these structures to determine the fillers of roles effectively without requiring excessive specialized information about each frame ;         score 3.5421977\n\nunderstanding election candidate approval ratings using social media data; the last few years has seen an exponential increase in the amount of social media data generated daily  thus  researchers have started exploring the use of social media data in building recommendation systems  prediction models  improving disaster management  discovery trending topics etc  an interesting application of social media is for the prediction of election results  the recently conducted 2012 us presidential election was the \"most tweeted\" election in history and provides a rich source of social media posts  previous work on predicting election outcomes from social media has been largely been based on sentiment about candidates  total volumes of tweets expressing electoral polarity and the like  in this paper we use a collection of tweets to predict the daily approval ratings of the two us presidential candidates and also identify topics that were causal to the approval ratings ;         score 1.7903347\n\nthe significance of bidding  accepting and opponent modeling in automated negotiation; given the growing interest in automated negotiation  the search for effective strategies has produced a variety of different negotiation agents  despite their diversity  there is a common structure to their design  a negotiation agent comprises three key components: the bidding strategy  the opponent model and the acceptance criteria  we show that this three component view of a negotiating architecture not only provides a useful basis for developing such agents but also provides a useful analytical tool  by combining these components in varying ways  we are able to demonstrate the contribution of each component to the overall negotiation result  and thus determine the key contributing components  moreover  we study the interaction between components and present detailed interaction effects  furthermore  we find that the bidding strategy in particular is of critical importance to the negotiator's success and far exceeds the importance of opponent preference modeling techniques  our results contribute to the shaping of a research agenda for negotiating agent design by providing guidelines on how agent developers can spend their time most effectively ;         score 1.7779316\n\nopponent modelling in texas hold'em poker as the key for success; over the last few years  research in artificial intelligence has focussed on games with incomplete information and non deterministic moves  the game of poker is a perfect theme for studying this subject  the best known poker variant is texas hold'em that combines simple rules with a huge amount of possible playing strategies  this paper is focussed on developing algorithms for performing simple online opponent modelling in texas hold'em poker enabling to select the best strategy to play against each given opponent  several autonomous agents were developed in order to simulate typical poker player's behaviour and an observer agent was developed  capable of using simple opponent modelling techniques  in order to select the best playing strategy against each opponent  the results obtained in realistic experiments using eight distinct poker playing agents showed the usefulness of the approach  the observer agent is clearly capable of outperforming all their counterparts in all tests performed ;         score 1.7645724\n\nthe influence of social networking sites on participation in the 2012 presidential election; social networking sites are gaining in popularity  and candidates for president have been getting more involved in these online platforms  in order to examine whether a presidential candidate's presence on social networking sites influences people's political participation  we conducted a survey asking users a series of questions related to their social networking involvement  political involvement  and political involvement on social networking sites  specifically with regard to the 2012 presidential election  our results indicate that despite being politically minded  these users do not use facebook for political reasons and a candidate's online presence does not influence their decision on how to vote ;         score 1.7614084\n\nquantifying political leaning from tweets and retweets; media outlets and pundits have been quick to embrace online social networks to disseminate their own opinions  but pundits’ opinions and news coverage are often marked by a clear political bias  as widely evidenced during the fiercely contested 2012 u s  presidential elections  given the wide availability of such data from sites like twitter  a natural question is whether we can quantify the political leanings of media outlets using osn data  in this work  by drawing a correspondence between tweeting and retweeting behavior  we formulate political leaning estimation as an ill posed linear inverse problem  the result is a simple and scalable approach that does not require explicit knowledge of the network topology  we evaluate our method with a dataset of 119 million election related tweets collected from april to november  and use it to study the political leaning of prominent tweeters and media sources ;         score 1.7574166\n\npower dynamics in spoken interactions: a case study on 2012 republican primary debates; in this paper  we explore how the power differential between participants of an interaction affects the way they interact in the context of political debates  we analyze the 2012 republican presidential primary debates where we model the power index of each candidate in terms of their poll standings  we find that the candidates' power indices affected the way they interacted with others in the debates as well as how others interacted with them ;         score 1.755458\n\na formal security model of a smart card web server; smart card web server provides a modern interface between smart cards and the external world  it is of paramount importance that this new software component does not jeopardize the security of the smart card  this paper presents a formal model of the smart card web server specification and the proof of its security properties  the formalization enables a thoughtful analysis of the specification that has revealed several ambiguities and potentially dangerous behaviors  our formal model is built using a modular approach upon a model of java card and global platform  by proving the security properties  we show that the smart card web server preserves the security policy of the overall model  in other words  this component introduces no illegal access to the card resources ( i e  file system and applications)  furthermore  the smart card web server provides a means for securely managing the card contents ( i e  resources update) ;         score 1.7514918\n\nproposing candidate views for materialization; view selection concerns selection of appropriate set of views for ma  terialization subject to constraints like size  space  time etc  however  selecting optimal set of views for a higher dimensional data set is an np hard problem  alternatively  views can be selected by exploring the search space in a greedy manner  several greedy algorithms for view selection exist in literature among which hrua is considered the most fundamental  hrua exhibits high run time complexity primarily because the number of possible views that it needs to evaluate is exponential in the number of dimensions  as a result  it would be  come infeasible to select views for higher dimensional data sets  the proposed views greedy algorithm (pvga)  presented in this paper  addresses this prob  lem by selecting views from a smaller set of proposed views  instead of all the views in the lattice as in case of hrua  this would make view selection more efficient and feasible for higher dimensional data  further  it was experimen  tally found that pvga trades significant improvement in time to evaluate all views with a slight drop in the quality of views selected for materialization ;         score 1.7505969\n\nintonation in political speech: ségolène royal vs  nicolas sarkozy; this study is not about the text or the rhetorical properties of the political discourse  but rather about the way the segments of text are assembled prosodically and delivered  the analysis of two short but typical examples pertaining to two former candidates to the french presidential elections held in 2007  i e  segolene royal sr and nicolas sarkozy ns  reveals that these leading political leaders differ in similar environment 1 in the complexity of the delivered prosodic structures  2 in the congruence or non congruence of prosody with syntax and 3 in the realization of continuation melodic contours  although one would expect a style more formal for ns and more relaxed for sr considering their respective political opinions ns close to the right  sr to the left  the prosodic style used by ns appears to be closer to the spontaneous speech used in non formal conditions  whereas sr consistently uses a very formal and conservative organization of her public speeches ;         score 1.74265\n\nbidflow: a new graph based bidding language for combinatorial auctions; in this paper we introduce a new graph based bidding language for combinatorial auctions  in our language  each bidder submits to the arbitrator a generalized flow network (netbid) representing her bids  the interpretation of the winner determination problem as an aggregation of individual preferences represented as flowbids allows building an aggregate netbid for its representation  labelling the nodes with appropriate procedural functions considerably improves upon the expressivity of previous bidding languages ;         score 1.7351379\n\nmassive media event data analysis to assess world wide political conflict and instability; mining massive daily news media data to infer patterns of cultural trends  including political conflicts and instabilities  is an important goal of computational social science and the new interdisciplinary field called \"culturnomics \" while the sheer size of media data makes this task challenging  a greater hurdle is the nonstationarity of data  manifested in several ways  which invalidates surge in media coverage as a reliable indicator of political change  we demonstrate the use of advanced statistical  information theoretic  and random fractal methods to analyze cameo encoded political events data  in particular  we show that on the country level  event distributions obey a zipf mandelbrot law  and interactions among countries follow an exponential law  indicating that local or prioritized events dominate the political environment of a country  most importantly  we find that world wide political instabilities  such as the arab spring  are associated with breakdown or enhancement of long range correlations in political events ;         score 1.7341329\n\nconvergence analysis for weighted joint strategy fictitious play in generalized second price auction; generalized second price (gsp) auction is one of the most commonly used auction mechanisms in sponsored search  as compared to conventional equilibrium analyses on gsp auctions  the convergence analysis on the dynamic behaviors of the bidders can better describe real world sponsored search systems  and give them a more useful guideline for making improvement  however  most existing works on convergence analysis assume the bidders to be greedy in taking actions  i e  they only utilize the bid information in the current round of auction when determining the best strategy for the next round  we argue that real world professional advertisers are more capable and can utilize the information in a longer history to optimize their strategies  accordingly  we propose modeling their behaviors by a weighted joint strategy fictitious play (wjsfp)  in the proposed model  bidders determine their optimal strategies based on their beliefs on other bidders' bid prices  and the beliefs are updated by considering all the information they have received so far in an iterative manner  we have obtained the following theoretical results regarding the proposed model: 1) when there are only two ad slots  the bid profile of the bidders will definitely converge  when there are multiple slots  there is a sufficient condition that guarantees the convergence of the bid profile  2) as long as the bid profile can converge  it converges to a nash equilibrium of gsp  to the best of our knowledge  this is the first time that the joint strategy fictitious play is adopted in such a complex game as sponsored search auctions ;         score 1.7317228\n\nidentification of the minimal set of attributes that maximizes the information towards the author of a political discourse: the case of the candidates in the mexican presidential elections; authorship attribution has attracted the attention of the natural language processing and machine learning communities in the past few years  here we are interested in finding a general measure of the style followed in the texts from the three main candidates in the mexican presidential elections of 2012  we analyzed dozens of texts (discourses) from the three authors  we applied tools from the time series processing field and machine learning community in order to identify the overall attributes that define the writing style of the three authors  several attributes and time series were extracted from each text  a novel methodology  based in mutual information  was applied on those time series and attributes to explore the relevance of each attribute to linearly separate the texts accordingly to their authorship  we show that less than 20 variables are enough to identify  by means of a linear recognizer  the authorship of a text from within one of the three considered authors ;         score 1.7293005\n\ndefining privacy for weighted votes  single and multi voter coercion; most existing formal privacy definitions for voting protocols are based on observational equivalence between two situations where two voters swap their votes  these definitions are unsuitable for cases where votes are weighted  in such a case swapping two votes can result in a different outcome and both situations become trivially distinguishable  we present a definition for privacy in voting protocols in the applied pi calculus that addresses this problem  using our model  we are also able to define multi voter coercion  i e  situations where several voters are attacked at the same time  then we prove that under certain realistic assumptions a protocol secure against coercion of a single voter is also secure against coercion of multiple voters  this applies for receipt freeness as well as coercion resistance ;         score 1.7273285\n"
     ]
    }
   ],
   "source": [
    "# hybrid v2\n",
    "size = 15\n",
    "\n",
    "query = clean_text(\"trump\")\n",
    "query_vec = vectorize_v3(stop_words, ln_supported, model, lang_utils, query)\n",
    "\n",
    "query_body = {\n",
    "  \"size\": size,\n",
    "  \"query\": {\n",
    "    \"function_score\": {\n",
    "      \"query\": {\n",
    "        \"match_all\": { }\n",
    "      },\n",
    "      \"functions\": [\n",
    "      {\n",
    "        \"filter\" : {\n",
    "          \"multi_match\": { \n",
    "            \"query\": query,\n",
    "            \"fields\": [\"data\", \"title\"]\n",
    "          }\n",
    "        },\n",
    "        \"weight\": 2,\n",
    "      },\n",
    "      {\n",
    "        \"script_score\" : {\n",
    "          \"script\" : {\n",
    "            \"source\": \"cosineSimilarity(params.query_value, doc[params.field1]) + cosineSimilarity(params.query_value, doc[params.field2])\",\n",
    "            \"params\": {\n",
    "              \"field1\": \"title_embeddings\",\n",
    "              \"field2\": \"data_embeddings\",\n",
    "              \"query_value\": query_vec\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"weight\": 1\n",
    "      }\n",
    "      ],\n",
    "      \"score_mode\": \"sum\",\n",
    "      \"boost_mode\": \"sum\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "res = es.search(index=\"toy_data_docs_embb\", body=query_body)\n",
    "print('\\n\\n'.join(['; '.join([res['hits']['hits'][i]['_source']['title'], res['hits']['hits'][i]['_source']['data'], f\"        score {res['hits']['hits'][i]['_score']}\"]) for i in range(len(res['hits']['hits']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sets ;         score 6.2425756\n\nstructure preserving runge kutta methods for stochastic hamiltonian equations with additive noise; there has been considerable recent work on the development of energy conserving one step methods that are not symplectic  here we extend these ideas to stochastic hamiltonian problems with additive noise and show that there are classes of runge kutta methods that are very effective in preserving the expectation of the hamiltonian  but care has to be taken in how the wiener increments are sampled at each timestep  some numerical simulations illustrate the performance of these methods ;         score 6.240659\n\nlocal and global intrinsic dimensionality estimation for better chemical space representation; in this paper  local and global intrinsic dimensionality estimation methods are reviewed  the aim of this paper is to illustrate the capacity of these methods in generating a lower dimensional chemical space with minimum information error  we experimented with five estimation techniques  comprising both local and global estimation methods  extensive experiments reveal that it is possible to represent chemical compound datasets in three dimensional space  further  we verified this result by selecting representative molecules and projecting them to 3d space using principal component analysis  our results demonstrate that the resultant 3d projection preserves spatial relationships among the molecules  the methodology has potential implications for chemoinformatics issues such as diversity  coverage  lead compound selection  etc ;         score 6.240303\n\nprotein subcellular location prediction based on pseudo amino acid composition and immune genetic algorithm; protein subcellular location prediction with computational method is still a hot spot in bioinformatics  in this paper  we present a new method to predict protein subcellular location  which based on pseudo amino acid composition and immune genetic algorithm  hydrophobic patterns of amino acid couples and approximate entropy are introduced to construct pseudo amino acid composition  immune genetic algorithm (iga) is applied to find the fittest weight factors for pseudo amino acid composition  which are crucial in this method  as such  high success rates are obtained by both self consistency test and jackknife test  more than 80% predictive accuracy is achieved in independent dataset test  the result demonstrates that this new method is practical  and  the method illuminates that the hydrophobic patterns of protein sequence influence its subcellular location ;         score 6.2389092\n\na modeling method and declarative language for temporal reasoning based on fluid qualities; current knowledge representation mechanisms focus more on providing a static description of a modeled universe and less on capturing evolution  ontology modeling languages  such as owl  have no inherent means for describing time or time dependent properties  in such settings  time is usually represented along with other applicationdependent concepts  yielding complex models that are difficult to maintain  extend  and reason about  on the other hand  in imperative languages that allow the definition of time dependent behavior and interactions such as ws bpel  the emphasis is on specifying the control flow in a service oriented environment  in contrast  we argue that a declarative approach is more suitable  we propose a modeling method and a declarative language  designed for representing and reasoning about time dependent properties  the method is applicable in areas such as ubiquitous computing  allowing the specification of intelligent device behaviour ;         score 6.238696\n\nadaptive evolutionary computation of the parametric optimization problem; the aim of the paper is to present a special type of adaptive evolutionary method  named here two level adaptive evolutionary computation (tlaec)  the method consists in combination of evolutionary computation with deterministic optimization algorithms in a hierarchy system  novelty of the method consists also in a new type of adaptation mechanism  post optimal analysis of the lower level optimization task is utilized in order to modify probability distribution for new genotype generations  the formal description of the method is presented in the paper  the application of this method to a mixed  discrete continuous linear optimization task is given as an example ;         score 6.2382565\n\nthe neural mechanism of human numerical inductive reasoning process: a combined erp and fmri study; inductive reasoning is one of the most important higher level cognitive functions of the human brain  and we still know very little about its neural mechanism  in the present study  event related potential (erp) and event related fmri are used to explore the dynamic spatiotemporal characteristics of inductive reasoning process  we hypothesize that the process of numerical inductive reasoning is partially dissociable over time and space  a typical task of inductive reasoning  function finding  was adopted  induction tasks and calculation tasks were performed in the experiments  respectively  erp results suggest that the time course of inductive reasoning process is partially dissociable as the following three sub processes: number recognition (the posterior p100 and n200)  strategy formation (p300) and hypothesis generation and verification (the positive slow waves)  fmri results show many activations  including prefrontal gyrus (ba 6)  inferior parietal lobule (ba 7  40)  and occipital cortex (ba 18)  after the respective discussions  the two kinds of data are combined qualitatively  then the dynamic spatiotemporal characteristic of inductive reasoning process are depicted using a conceptual figure  this study is a preliminary effort towards deeply understanding the dynamic information processing mechanism of human inductive reasoning process ;         score 6.238017\n\ndeductive modeling of human cognition; deductive analysis (da) is presented as an approach to the study and simulation of human cognitive processes  da is composed of a psychological theory and a methodology that can sheed light on mental phenomenas  the cognitive theory is embedded in the problem space and the control structure hypotheses  the methodology consists of logical derivations of computer models from an abstract specification  the item recognition task and the three term series task are analyzed for purpose of illustration  several aspects of human cognition in these environments are discussed  it is argued that da brings new notions to the study of human cognition  for instance  to design sets of models and to distinguish between empirically equivalent models ;         score 6.237927\n\ninterval probabilities of state transitions in probabilistic automata; working principle of the probabilistic automaton is based on the state transition probability [4]  [6]  construction of the automaton depends on its purpose  we assume that all transitions probabilities are constant[3]  in situations where low levels of information (entropy) suggest assurance solutions  we use interval probabilities (remembering that the sum of the probabilities of transition to the given state must remain equal to 1)  this variant generates additional problems  which are: how to prevent the redundancy of the probabilities sum  the physical interpretation of this situation  undervaluing of the probabilities sum  determine the criteria of the structure and parameters optimization (minimization) relative to the semantic resources of formal languages described by the automaton  change in the level of information entropy ;         score 6.2377043\n\nstructure and electromagnetic actuation systems of microrobot; according to the feature of the magnetic actuation principle  now there are mainly three driving methods: one is alternating magnetic field  another is stationary gradient magnetic field and the last one is rotating magnetic field  this paper provides a detailed insight into the present day state of microrobot technology and development on structure and electromagnetic actuation (ema) systems  firstly  the microrobot based on the three actuation methods is selectively elaborated respectively and the structure of ema systems  microrobot characteristics and actuation principle are all analyzed and compared  finally some of the critical aspects of microrobot and ema system design are considered  this paper shows that rotating magnetic field will be the most actuation promising method in the future biomedical application ;         score 6.237504\n\nmodelling the acitivation of words in human memory: the spreading activation  spooky activation at a distance and the entanglement models compared; modelling how a word is activated in human memory is an important requirement for determining the probability of recall of a word in an extra list cueing experiment  the spreading activation  spooky action at a distance and entanglement models have all been used to model the activation of a word  recently a hypothesis was put forward that the mean activation levels of the respective models are as follows: spreading ≤ entanglment ≤ spooking action at a distance this article investigates this hypothesis by means of a substantial empirical analysis of each model using the university of south florida word association  rhyme and word norms ;         score 6.237372\n\nan interval type 2 fuzzy logic system for human silhouette extraction in dynamic environments; in this paper  we present a type 2 fuzzy logic based system for robustly extracting the human silhouette which is a fundamental and important procedure for advanced video processing applications  such as pedestrian tracking  human activity analysis and event detection  the presented interval type 2 fuzzy logic system is able to detach moving objects from extracted human silhouette in dynamic environments  our real world experimental results demonstrate that the proposed interval type 2 fuzzy logic system works effectively and efficiently for moving objects detachment where the type 2 approach outperforms the type 1 fuzzy system while significantly reducing the misclassification when compared to the type 1 fuzzy system ;         score 6.235312\n\nthe use of continuity in a qualitative physics; the ability to reason about a series of complex events over time is essential in analyzing physical systems  this paper discusses the role of continuity in qualitative physics and its application in a system for analyzing the behavior of digital mos circuits that exhibit analog behavior  the discussion begins with a brief overview of the reasoning steps necessary to perform a qualitative simulation using temporal qualitative (tq) analysis  the discussion then focuses in on the use of continuity and the relationship between quantities and their higher order derivatives in describing how physical quantities change over time ;         score 6.235197\n\nimproving gaussian process value function approximation in policy gradient algorithms; the use of value function approximation in reinforcement learning (rl) problems is widely studied  the most common application of it being the extension of value based rl methods to continuous domains  gradient based policy search algorithms can also benefit from the availability of an estimated value function  as this estimation can be used for gradient variance reduction  in this article we present a new value function approximation method that uses a modified version of the kullback leibler (kl) distance based sparse on line gaussian process regression  we combine it with williams' episodic reinforce algorithm to reduce the variance of the gradient estimates  a significant computational overload of the algorithm is caused by the need to completely re estimate the value function after each gradient update step  to overcome this problem we propose a measure composed of a kl distance based score and a time dependent factor to exchange obsolete basis vectors with newly acquired measurements  this method leads to a more stable estimation of the action value function and also reduces gradient variance  performance and convergence comparisons are provided for the described algorithm  testing it on a dynamic system control problem with continuous state action space ;         score 6.2350926\n\ngray scale potential theory of sparse image; according to the relative position among the pixels of sparse image  we proposed the gray scale potential of image  by taking the example of the binary images  this paper highlighted the definition of gray scale potential and the extraction of gray scale potential  then we pointed out that the gray scale potential was an intrinsic feature of image  as for binary image  it reflects the relative distances of pixels to a baseline or to a reference point  and if the image is gray image  it reflects not only the distances but also the gray level feature  the gray scale potential has obvious advantage in representing the sparse image  because it can reduce the computational work and storage  even two dimensional image can be simplified to one dimensional curve  finally  some experimental data were given to illustrate the concept of gray scale potential  it shows that the gray scale potential of image is a steady feature and can be used in object recognition ;         score 6.234805\n\nlindig's algorithm for concept lattices over graded attributes; formal concept analysis (fca) is a method of exploratory data analysis  the data is in the form of a table describing relationship between objects (rows) and attributes (columns)  where table entries are grades representing degrees to which objects have attributes  the main output of fca is a hierarchical structure (so called concept lattice) of conceptual clusters (so called formal concepts) present in the data  this paper focuses on algorithmic aspects of fca of data with graded attributes  namely  we focus on the problem of generating efficiently all clusters present in the data together with their subconcept superconcept hierarchy  we present theoretical foundations  the algorithm  analysis of its efficiency  and comparison with other algorithms ;         score 6.234543\n\nhybrid analytical and ann based modelling of temperature sensors nonlinear dynamic properties; the paper presents new methods for modelling of temperature sensors' dynamics by means of artificial neural networks (ann) and hybrid analytical neural approach  feedforward multilayer ann and a moving window method  as well as recurrent neural networks (rnn) are applied  the proposed modelling techniques were evaluated experimentally for two small platinum resistance temperature detectors (rtds) immersed in oil  experiments were performed in temperature range  for which the sensors characteristics are nonlinear  the proposed ann based and hybrid analytical neural models were validated by means of computer simulations on the basis of the quality of dynamic errors correction  it was shown that in the process conditions for which classical methods and linear models fail  the application of anns and hybrid techniques which combine soft and hard computing paradigms can significantly improve modelling quality ;         score 6.234532\n\nphysics based modeling of aortic wall motion from ecg  gated 4d computed tomography; recent advances in electrocardiogram (ecg) gated computed tomography (ct) technology provide 4d (3d+t) information of aortic wall motion in high spatial and temporal resolution  however  imaging artifacts  e g  noise  partial volume effect  misregistration and/or motion blurring may preclude its usability in many applications where accuracy and reliability are concerns  although it is possible to find correspondence through tagged mri or echo or image registration  it may be either inconsistent to the physics or difficult to utilize data from all frames  in this paper  we propose a physics based filtering approach to construct a dynamic model from these 4d images  it includes a state filter that corrects simulated displacements from an elastic finite element model to match observed motion from images  in the meantime  the model parameters are refined to improve the model quality by applying a parameter filter based on ensemble kalman filtering  we evaluated the performance of our method on synthetic data where ground truths are available  finally  we successfully applied the method to a real data set ;         score 6.234506\n\npolyphonic transcription: exploring a hybrid of tone models and particle swarm optimisation; polyphonic transcription could be formulated as a supervised classification task if the classifiers of all possible polyphonic combinations could be learned beforehand  however  it is impractical to learn all possible classification models in real life due to the exponential explosion of all possible polyphonic combinations  here  we describe a novel polyphonic transcription approach that applies a hybrid of the particle swarm optimisation (pso) and the tone model techniques  this hybrid approach exploits the strengths from both the heuristic search and the model based approaches  in our work  only the monophonic tone models of all pitches are learned and employed to calculate the first pass output of polyphonic transcription  which is then refined in the second pass by pso  the experimental results show that the proposed hybrid approach outperform the competing non negative matrix factorisation (nmf) approach  this paper presents and discusses the design and the experimental results of this novel approach ;         score 6.233314\n\nan empirical study on interpretability indexes through multi objective evolutionary algorithms; in the realm of fuzzy systems  interpretability is really appreciated in most applications  but it becomes essential in those cases in which an intensive human machine interaction is necessary  accuracy and interpretability are often conflicting goals  thus we used multiobjective fuzzy modeling strategies to look for a good trade off between them  for assessing interpretability  two different interpretability indexes have been taken into account: average fired rules (afr)  which estimates how simple the comprehension of a specific rule base is  and logical view index (lvi)  which estimates how much a rule base satisfies logical properties  with the aim of finding possible relationships between afr and lvi  they have been used in two independent experimental sessions against the classification error  experimental results have shown that the afr minimization implies the lvi minimization  while the opposite is not verified ;         score 6.232691\n\nmulti view maximum entropy discrimination; maximum entropy discrimination (med) is a general framework for discriminative estimation based on the well known maximum entropy principle  which embodies the bayesian integration of prior information with large margin constraints on observations  it is a successful combination of maximum entropy learning and maximum margin learning  and can subsume support vector machines (svms) as a special case  in this paper  we present a multi view maximum entropy discrimination framework that is an extension of med to the scenario of learning with multiple feature sets  different from existing approaches to exploiting multiple views  such as co training style algorithms and co regularization style algorithms  we propose a new method to make use of the distinct views where classification margins from these views are required to be identical  we give the general form of the solution to the multi view maximum entropy discrimination  and provide an instantiation under a specific prior formulation which is analogical to a multi view version of svms  experimental results on real world data sets show the effectiveness of the proposed multi view maximum entropy discrimination approach ;         score 6.2321196\n"
     ]
    }
   ],
   "source": [
    "# hybrid v3\n",
    "size = 500\n",
    "\n",
    "query = clean_text(\"cosmologique\")\n",
    "query_vec = vectorize_v3(stop_words, ln_supported, model, lang_utils, query)\n",
    "\n",
    "query_body = {\n",
    "  \"size\": size,\n",
    "  \"query\": {\n",
    "    \"function_score\": {\n",
    "      \"query\": {\n",
    "        \"bool\": { \n",
    "          \"should\" : [\n",
    "            {\n",
    "              \"multi_match\" : { \n",
    "                \"query\": query,\n",
    "                \"fields\": [\"data\", \"title\"]\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"match_all\": { }\n",
    "            }\n",
    "          ],\n",
    "          \"minimum_should_match\" : 0\n",
    "        }\n",
    "      },\n",
    "      \"functions\": [\n",
    "      {\n",
    "        \"script_score\" : {\n",
    "          \"script\" : {\n",
    "            \"source\": \"cosineSimilarity(params.query_value, doc[params.field1]) + cosineSimilarity(params.query_value, doc[params.field2])\",\n",
    "            \"params\": {\n",
    "              \"field1\": \"title_embeddings\",\n",
    "              \"field2\": \"data_embeddings\",\n",
    "              \"query_value\": query_vec\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"weight\": 5\n",
    "      }\n",
    "      ],\n",
    "      \"score_mode\": \"sum\",\n",
    "      \"boost_mode\": \"sum\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "res = es.search(index=\"toy_data_docs_embb\", body=query_body)\n",
    "print('\\n\\n'.join(['; '.join([res['hits']['hits'][i]['_source']['title'], res['hits']['hits'][i]['_source']['data'], f\"        score {res['hits']['hits'][i]['_score']}\"]) for i in range(len(res['hits']['hits']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3523\n"
     ]
    }
   ],
   "source": [
    "print(res['took'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3d model based segmentation of 3d biomedical images; a central task in biomedical image analysis is the segmen  tation and quantification of 3d image structures  a large variety of seg  mentation approaches have been proposed including approaches based on different types of deformable models  a main advantage of deformable models is that they allow incorporating a priori information about the considered image structures  in this contribution we give a brief overview of often used deformable models such as active contour models  statisti  cal shape models  and analytic parametric models  moreover  we present in more detail 3d analytic parametric intensity models  which enable accurate and robust segmentation and quantification of 3d image struc  tures  such parametric models have been successfully used in different biomedical applications  for example  for the localization of 3d anatom  ical point landmarks in 3d mr and ct images  for the quantification of vessels in 3d mra and cta images  as well as for the segmentation of cells and subcellular structures in 3d microscopy images ;         score 9.885883\n\nreconstructing 3d face shapes from single 2d images using an adaptive deformation model; the representational power (rp) of an example based model is its capability to depict a new 3d face for a given 2d face image  in this contribution  a novel approach is proposed to increase the rp of the 3d reconstruction pca based model by deforming a set of examples in the training dataset  by adding these deformed samples together with the original training samples we gain more rp  a 3d pca based model is adapted for each new input face image by deforming 3d faces in the training data set  this adapted model is used to reconstruct the 3d face shape for the given input 2d near frontal face image  our experimental results justify that the proposed adaptive model considerably improves the rp of the conventional pca based model ;         score 9.536514\n\nimm algorithm for a 3d high maneuvering target tracking; a major challenge posed by target tracking problems is the target flying at high speeds and performing \"high g\" turns in the 3d space  in this situation  horizontally or decoupled models may lead to an unacceptable accuracy  to address this problem an imm algorithm that includes a 3d constant velocity model (cv)  a 3d \"current\" statistic model (csm)  and 3d constant speed coordinated turn model (3dcsct) with the kinematic constant for constant speed targets is proposed  the tracking performance of the proposed imm algorithm is compared with that of an imm algorithm utilizing cv  3dcsct  and constant acceleration model (ca) or singer model  simulation results that demonstrate the algorithm is feasible and practical for a 3d high maneuvering target tracking ;         score 9.532935\n\na novel 3d partitioned active shape model for segmentation of brain mr images; a 3d partitioned active shape model (pasm) is proposed in this paper to address the problems of the 3d active shape models (asm)  when training sets are small  it is usually the case in 3d segmentation  3d asms tend to be restrictive  this is because the allowable region spanned by relatively few eigenvectors cannot capture the full range of shape variability  the 3d pasm overcomes this limitation by using a partitioned representation of the asm  given a point distribution model (pdm)  the mean mesh is partitioned into a group of small tiles  in order to constrain deformation of tiles  the statistical priors of tiles are estimated by applying principal component analysis to each tile  to avoid the inconsistency of shapes between tiles  training samples are projected as curves in one hyperspace instead of point clouds in several hyperspaces  the deformed points are then fitted into the allowable region of the model by using a curve alignment scheme  the experiments on 3d human brain mris show that when the numbers of the training samples are limited  the 3d pasms significantly improve the segmentation results as compared to 3d asms and 3d hierarchical asms ;         score 9.314356\n\n3d gesture based view manipulator for large scale entity model review; hand gesture based human computer interaction (hci) is one of the most natural and intuitive methods of communication between humans and machines because it closely mimics how humans interact with each other  its intuitiveness and naturalness are needed to explore extensive and complex data or virtual realities  we developed a 3d gesture interface to manipulate the display of a 3d entity model  for gesture recognition  we use the kinect as a depth sensor to acquire depth image frames  we track the position of the user's skeleton in each frame and detect preset gestures  by simple gestures  the user can pan  zoom  rotate  and reset the view and freely navigate inside the 3d entity model in the virtual space  the proposed gesture interface is integrated with the stereoscopic 3d model viewer that we have previously developed for 3d model review ;         score 9.248587\n\narchitectural reconstruction with multiple views and geometric constraints; we present a supervised approach to recover 3d models of buildings from multiple uncalibrated views  with this method the user matches 3d vertices in the images and defines the 3d model of the building with the help of elementary and intuitive geometric constraints  at the same time  a graph describing relationships between vertices is built  then  unknown parameters of this graph are estimated non linearly through a bundle adjustment to recover the building model and the camera parameters  this method asserts that geometric rules are perfectly respected  this approach is used to recover independently 3d parts of the building with suitable images  then all these independent 3d models are merged to obtain a full multiscale model of the building  an example on real images is given ;         score 9.245668\n\nusing the orthographic projection model to approximate the perspective projection model for 3d facial reconstruction; this study develops a 3d facial reconstruction system  which consists of five modules  using the orthographic projection model to approximate the perspective projection model  the first module identifies a number of feature points on the face and tracks these feature points over a sequence of facial images by the optical flow technique  the second module applies the factorization method to the orthographic model to reconstruct a 3d human face  the facial images are acquired using a pinhole camera  which are based on a perspective projection model  however  the face is reconstructed using an orthographic projection model  to compensate for the difference between these two models  the third module implements a simple and efficient method for approximating the perspective projection model  the fourth module overcomes the missing point problem  commonly arising in 3d reconstruction applications  finally  the fifth module implements a smoothing process for the 3d surface by interpolating additional vertices ;         score 9.064151\n\nargo vehicle simulation of motion driven 3d lidar detection and environment awareness; this paper presents a method to detect three dimensional (3d) objects using an occupancy grid based mapping technique  this paper adapts the 3d occupancy grid based mapping technique from the two dimensional (2d) occupancy grid based mapping technique commonly used in simultaneous localization and mapping applications  the 3d occupancy mapping technique uses a 3d inverse measurement model and has been developed for a lidar based off road ground rover vehicle that drives on 3d terrain  the technique is developed and simulated in matlab to demonstrate its 3d object detection capabilities  this technique was developed as part of off road autonomous vehicle research being conducted at the university of waterloo ;         score 8.809802\n\nreconstructing 3d boundary element heart models from 2d biplane fluoroscopy; individual 3d boundary element models can be used in solving inverse problems in electro  and magnetocardiographic measurements  in some cases 3d data  such as magnetic resonance (mr) or computed tomography (ct) images  are not available  therefore  it would be useful to be able to use 2d images such as x ray projections for creating 3d models  the aim of this work was to develop a software package for creating a 3d boundary element heart model from two orthogonal x ray projections  the biplane fluoroscopy images from a patient are digitized and the images are enhanced with different image processing techniques  the patient heart outline is segmented from the x ray projections  the outline is compared with virtual x ray projections created from a prior 3d model segmented from mr images  the difference between the outlines is used to deform the prior model  the quality of the digitized x ray projections was noticeably improved and thus the heart outline segmentation was facilitated  the deformation method implemented is robust and provides good results even when the source parameters contain errors ;         score 8.73469\n\nfrom real to virtual rapid architectural prototyping; can greater visual realism of a real time architectural virtual walkthrough achieve similar high sensory impact  or qualia  as a fabricated 3d printed scale model of an urban landscape  the aim of this project is to answer that question by allowing a real existing city heritage landscape during a large urban planning project to be 3d modeled and subsequently be studied via a dual output: a fabricated real  physical scale model based on a latest high quality color 3d printer and an equivalent 3d virtual walkthrough of enhanced real time visual realism based on a recent game engine  conclusions of this experiment and user study suggest that a virtual  interactive simulation based on specific latest real time rendering algorithms can indeed convey a similar user experience and feeling of \"presence\" that an equivalent architectural scale model offers  regarding fast appreciation of both space and structure ;         score 8.665028\n\n3d articulated hand tracking based on behavioral model; taken it into consideration that human has a great deal of experiences and knowledge of hand postures  if these operating skills of postures are applied to hci  the simple and convenient human computer interface can be expected  in fact  tracking  recognition and interaction based on 3d freehand are a part of the cores in our virtual assembly system  but it is a challenging task to track 3d freehand in real time because of high dimensionality of 3d full hand model  a novel framework for 3d freehand tracking is put forward in this paper  firstly  we model and investigate this problem under our virtual assembly system (vas)  so as to decrease the arbitrariness and complexity of this issue  secondly  we put emphasis on building cognitive and behavioral model (cbm) for users in vas  thirdly  we research on the way to track 3d freehand based on cbm  the main contributions of this paper are that we propose a new cbm  tptm model  provide a way to connect users and computer for effective interaction  and present a real time freehand tracking algorithm  based on tptm model  the prediction  the number of particles  the way and scope of sampling  are optimized  tptm model not only explain behavioral characteristics for users but also can effectively guide the design of freehand tracking algorithm  tptm model also provides a data structure that can facilitate the implementation of the tracking algorithm  our experimental results show that the proposed approach raises the quality of each sampled particle or avoid sampling \"poor\" particles which appear with low probability in each frame  and it tracks 3d freehand in real time with high accuracy  the number of the drawn particles is reduced up to 5 and the tracking speed increase up to 81 ms per frame ;         score 8.6473675\n\na new algorithm for 3d shape recognition by means of the 2d point distance histogram; a new algorithm for the recognition of three dimensional objects is proposed in this paper  the algorithm is based on the rendering of several 2d projections of a 3d model  from various positions of the camera  similarly to the proposition given in [1]  the vertices of the dodecahedron enclosing the processed model contain the cameras for this purpose  the obtained projections are stored in bitmaps and the point distance histogram for the description of the planar shapes extracted from them is applied  the obtained histograms represent a 3d model  the experiments performed have confirmed the high efficiency of the proposed algorithm  it outperformed five other algorithms for the representation of 3d shapes ;         score 8.626605\n\n3d expressive face model based tracking algorithm; this paper presents a method for tracking a face on a video sequence  by recovering the full motion and the expression deformation of the face using 3d expressive facial model  from some characteristic face points given on the first frame  an approximated 3d model of the face is reconstructed  using a steepest descent image approach  the algorithm is able to extract simultaneously the parameters related to the face expression and to the 3d posture  the algorithm has been tested on the kanade cohn database [1] and its precision has been compared with a standard multicamera system for the 3d tracking (elite2002 system)  the results in both cases are good  the proposed approach is part of a facial expression analysis system  our aim is to detect the facial expressions in situations characterized by a moderate head motion in realistic experimental conditions (illumination from the ceiling  and subjects not in frontal pose) ;         score 8.594103\n\nautomated generation of an historic 4d city model of hamburg and its visualisation with the ge engine; current 3d city models are already available for many cities world wide  however  the production of historical city models is still in its infancy  in this paper a procedure is presented that combines different data sources in order to derive individual 3d city models of different time periods using the example of the free and hanseatic city of hamburg  a wooden model of the city from the year 1644 and an official map from 1859 have been used as a basis for the generation of the 4d city model  the physical model (~1:1000) was scanned by a fringe projection system for 3d modelling  while the digitized data from the map were combined with height information from different data sources  these two geo referenced 3d city models were used to derive further epochs (1200  1400  1589 and 1700) using different historical bird's eye views (isometric views) of the city  for interactive navigation and visualization of the 4d city model a program was developed using the google earth application programming interface ;         score 8.589582\n\n3d shape analysis for early diagnosis of malignant lung nodules; an alternative method for diagnosing malignant lung nodules by their shape rather than conventional growth rate is proposed  the 3d surfaces of the detected lung nodules are delineated by spherical harmonic analysis  which represents a 3d surface of the lung nodule supported by the unit sphere with a linear combination of special basis functions  called spherical harmonics (shs)  the proposed 3d shape analysis is carried out in five steps: (i) 3d lung nodule segmentation with a deformable 3d boundary controlled by two probabilistic visual appearance models (the learned prior and the estimated current appearance one)  (ii) 3d delaunay triangulation to construct a 3d mesh model of the segmented lung nodule surface  (iii) mapping this model to the unit sphere  (iv) computing the shs for the surface  and (v) determining the number of the shs to delineate the lung nodule  we describe the lung nodule shape complexity with a new shape index  the estimated number of the shs  and use it for the k nearest classification to distinguish malignant and benign lung nodules  preliminary experiments on 327 lung nodules (153 malignant and 174 benign) resulted in the 93 6% correct classification (for the 95% confidence interval)  showing that the proposed method is a promising supplement to current technologies for the early diagnosis of lung cancer ;         score 8.580928\n"
     ]
    }
   ],
   "source": [
    "# classic match to compare\n",
    "query = \"3d model\"\n",
    "query_vec = vectorize_v3(stop_words, ln_supported, model, lang_utils, clean_text(query))\n",
    "\n",
    "query_body = {\n",
    "    \"size\": 15,\n",
    "    \"query\": {\n",
    "      \"multi_match\": { \n",
    "            \"query\": query,\n",
    "            \"fields\": [\"data\", \"title\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "res = es.search(index=\"toy_data_docs_embb\", body=query_body)\n",
    "print('\\n\\n'.join(['; '.join([res['hits']['hits'][i]['_source']['title'], res['hits']['hits'][i]['_source']['data'], f\"        score {res['hits']['hits'][i]['_score']}\"]) for i in range(len(res['hits']['hits']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}