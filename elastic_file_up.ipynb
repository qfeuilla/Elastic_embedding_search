{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "80d14a88649dd46d429e21cbc1382463c00f035393205a877ff7ce1db51cc502"
   }
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/qfeuilla/.local/lib/python3.8/site-packages/elasticsearch/connection/http_requests.py:139: UserWarning: Connecting to https://localhost:9200 using SSL with verify_certs=False is insecure.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import string\n",
    "import re\n",
    "import io\n",
    "import ssl\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch.connection import create_ssl_context\n",
    "import urllib3\n",
    "import certifi\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "\n",
    "es = Elasticsearch('https://admin:admin@localhost:9200', \n",
    "                   verify_certs=False,\n",
    "                   connection_class=RequestsHttpConnection)\n",
    "\n",
    "# repalce the path with your json data file\n",
    "df = pd.read_csv(\"./data_mirion.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0                                         elastic_id  \\\n",
       "0           0  mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...   \n",
       "1           1  mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...   \n",
       "2           2  mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...   \n",
       "3           3  mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...   \n",
       "4           4  mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...   \n",
       "\n",
       "                                   id  parents_id  type    source categorie  \\\n",
       "0  013AHDHCP6OVIY5ACYNZDKMBZRMROW7FKV         NaN  file  onedrive   unknown   \n",
       "1  013AHDHCI4CM6Q74YA5JCIKDPUX3JOURZE         NaN  file  onedrive   unknown   \n",
       "2  013AHDHCOYR6DCXLYLOZAIWZZRSENIOE2P         NaN  file  onedrive   unknown   \n",
       "3  013AHDHCJTSN22C44RZBAYI6GSG4LKA3RM         NaN  file  onedrive   unknown   \n",
       "4  013AHDHCIPBKIJEXTR4RB2NBO5RLV5URW2         NaN  file  onedrive   unknown   \n",
       "\n",
       "          title content  \n",
       "0  DSC05908.JPG          \n",
       "1  DSC06024.JPG          \n",
       "2  DSC05936.JPG          \n",
       "3  DSC05881.JPG          \n",
       "4  DSC05856.JPG          "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>elastic_id</th>\n      <th>id</th>\n      <th>parents_id</th>\n      <th>type</th>\n      <th>source</th>\n      <th>categorie</th>\n      <th>title</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...</td>\n      <td>013AHDHCP6OVIY5ACYNZDKMBZRMROW7FKV</td>\n      <td>NaN</td>\n      <td>file</td>\n      <td>onedrive</td>\n      <td>unknown</td>\n      <td>DSC05908.JPG</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...</td>\n      <td>013AHDHCI4CM6Q74YA5JCIKDPUX3JOURZE</td>\n      <td>NaN</td>\n      <td>file</td>\n      <td>onedrive</td>\n      <td>unknown</td>\n      <td>DSC06024.JPG</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...</td>\n      <td>013AHDHCOYR6DCXLYLOZAIWZZRSENIOE2P</td>\n      <td>NaN</td>\n      <td>file</td>\n      <td>onedrive</td>\n      <td>unknown</td>\n      <td>DSC05936.JPG</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...</td>\n      <td>013AHDHCJTSN22C44RZBAYI6GSG4LKA3RM</td>\n      <td>NaN</td>\n      <td>file</td>\n      <td>onedrive</td>\n      <td>unknown</td>\n      <td>DSC05881.JPG</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>mcsftgraph-b!dE3xuXs5AU6rUgKa4HoozOg2AmqesWZGq...</td>\n      <td>013AHDHCIPBKIJEXTR4RB2NBO5RLV5URW2</td>\n      <td>NaN</td>\n      <td>file</td>\n      <td>onedrive</td>\n      <td>unknown</td>\n      <td>DSC05856.JPG</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unnamed: 0  removed\nelastic_id  removed\nid  removed\nparents_id  removed\ntype  removed\nsource  removed\ncategorie  removed\n"
     ]
    }
   ],
   "source": [
    "# this var should be the col you want to use for autocompletion\n",
    "col_to_keep = [\"title\", \"content\"]\n",
    "\n",
    "for c in df.columns:\n",
    "    if c not in col_to_keep:\n",
    "        print(c, \" removed\")\n",
    "        df = df.drop(columns=c)\n",
    "df.dropna(inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                        title  \\\n",
       "1169  U3x56 - Planning MES_KRT CT1_080813.pdf   \n",
       "1173                 E01004477& E01004492.pdf   \n",
       "1196                 U3x56-DED-14-180-PDL.doc   \n",
       "1198                            Bdx_19041.pdf   \n",
       "1217                 BE PC supp Tricastin.doc   \n",
       "\n",
       "                                                content  \n",
       "1169  N° Nom de la tâche Durée Début Fin 1 U30000056...  \n",
       "1173                                                     \n",
       "1196  EDF CIPN\\nService SEI\\n140, avenue Viton\\n1300...  \n",
       "1198  EDF DIPDE  140, Av VITON 13401 MARSEILLE A l'a...  \n",
       "1217  Adresse de livraison / delivery address\\n     ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1169</th>\n      <td>U3x56 - Planning MES_KRT CT1_080813.pdf</td>\n      <td>N° Nom de la tâche Durée Début Fin 1 U30000056...</td>\n    </tr>\n    <tr>\n      <th>1173</th>\n      <td>E01004477&amp; E01004492.pdf</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1196</th>\n      <td>U3x56-DED-14-180-PDL.doc</td>\n      <td>EDF CIPN\\nService SEI\\n140, avenue Viton\\n1300...</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>Bdx_19041.pdf</td>\n      <td>EDF DIPDE  140, Av VITON 13401 MARSEILLE A l'a...</td>\n    </tr>\n    <tr>\n      <th>1217</th>\n      <td>BE PC supp Tricastin.doc</td>\n      <td>Adresse de livraison / delivery address\\n     ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.replace(\" \", float(\"NaN\"), inplace=True)\n",
    "df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "df.dropna(subset = [\"content\", \"title\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\ndef singularize(word):\\n    return word[:-1] if len(word) > 0 and word[-1] == 's' and (len(word) > 2 and word[-2] != 'i') else word\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "'''\n",
    "import inflect\n",
    "\n",
    "# used for semantic preprocessing\n",
    "p = inflect.engine()\n",
    "\n",
    "# kind of long maybe find another solution ?\n",
    "def singularize(word):\n",
    "    a = p.singular_noun(word)\n",
    "    return a if a is not False else word\n",
    "'''\n",
    "\n",
    "# faster but less semantic\n",
    "'''\n",
    "def singularize(word):\n",
    "    return word[:-1] if len(word) > 0 and word[-1] == 's' and (len(word) > 2 and word[-2] != 'i') else word\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use a list of common word by language (like the, for ...) and remove them from the string in this function\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    take a str and return a preprocessed str\n",
    "    current preprocessing are :\n",
    "    lowercase\n",
    "    punctuation removal\n",
    "    singularization -> not anymore too long and dont give good results\n",
    "    '''\n",
    "    words = text.split()\n",
    "    stripped =  [re.sub(r\"[,.;@#?!&$-]+\\ *\", \" \", w).lower() for w in words]\n",
    "    res = \" \".join(stripped)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [00:01<00:00, 290.18it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = []\n",
    "pbar = tqdm(total=df.size // 2)\n",
    "for item in df.iterrows():\n",
    "    item = item[1]\n",
    "    corpus.append((clean_text(item[1]), clean_text(item[0])))\n",
    "\n",
    "    pbar.update()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('n° nom de la tâche durée début fin 1 u30000056   chaînes ddb 0 jou r mer 01/05/13 mer 01/05/13 2 tn 143 jours mer 15/05/13 ven 29/11/13 3 réunion enclenchement 1 jou r mer 15/05/13 mer 15/05/13 4 réception matériel sur site 0 jou r lun 29/07/13 lun 29/07/13 5 tn1   mes ddb hors bk 8 jours lun 26/08/13 mer 04/09/13 6 tn1   mes bk 4 jours mar 12/11/13 ven 15/11/13 7 tn2   mes bk 4 jours mar 05/11/13 ven 08/11/13 8 tn3   mes bk 4 jours mar 26/11/13 ven 29/11/13 9 tn4   mes bk 4 jours mar 19/11/13 ven 22/11/13 10 bu 183 jours mer 22/05/13 ven 31/01/14 11 réunion enclenchement 1 jou r mer 22/05/13 mer 22/05/13 12 réception matériel sur site 0 jou r ven 05/07/13 ven 05/07/13 13 bu3   mes ddb hors bk 5 jours lun 29/07/13 ven 02/08/13 14 bu3   mes bk 5 jours lun 15/07/13 ven 19/07/13 15 bu3   mes retour dal10 2 jours mar 20/08/13 mer 21/08/13 16 réception matériel bu2 sur site 0 jou r lun 06/01/14 lun 06/01/14 17 bu2   mes bk 5 jours lun 27/01/14 ven 31/01/14 18 gr 58 jours mer 12/06/13 ven 30/08/13 19 réunion enclenchement 1 jou r mer 12/06/13 mer 12/06/13 20 réception matériel sur site 0 jou r mer 14/08/13 mer 14/08/13 21 gr2   mes bk 5 jours lun 26/08/13 ven 30/08/13 22 ba 65 jours jeu 11/07/13 mer 09/10/13 23 réunion enclenchement 1 jou r jeu 11/07/13 jeu 11/07/13 24 réception matériel sur site 0 jou r lun 26/08/13 lun 26/08/13 25 ba2   mes ddb hors bk 8 jours lun 30/09/13 mer 09/10/13 26 ba1   mes bk 5 jours lun 09/09/13 ven 13/09/13 27 ba3   mes bk 5 jours lun 16/09/13 ven 20/09/13 28 cs 50 jours jeu 01/08/13 mer 09/10/13 29 réunion enclenchement 1 jou r jeu 01/08/13 jeu 01/08/13 30 réception matériel sur site 0 jou r ven 20/09/13 ven 20/09/13 31 cs2   mes ddb hors bk 8 jours lun 30/09/13 mer 09/10/13 01/05 29/07 05/07 06/01 14/08 26/08 20/09 22 29 06 13 20 27 03 10 17 24 01 08 15 22 29 05 12 19 26 02 09 16 23 30 07 14 21 28 04 11 18 25 02 09 16 23 30 06 13 20 27 03 mai 13 jui 13 jul 13 aoû 13 sep 13 oct 13 nov 13 déc 13 jan 14 fév 1 tâche fractionnement avancement jalon récapitulative récapitulatif du projet tâches externes jalons externes échéance page 1 projet : planning mes_krt ct date : jeu 08/08/13', 'u3x56   planning mes_krt ct1_080813 pdf')\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "utils = [\"embedd\", \"id2word\", \"word2id\"]\n",
    "ln_supported = [\"fr\"]\n",
    "\n",
    "lang_utils = { ln: {utils[i]: d for i, d in enumerate(load_vec(f'wiki.multi.{ln}.vec'))} for ln in tqdm(ln_supported)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/qfeuilla/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = {\"fr\": set(stopwords.words('french'))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    return cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndef vectorize_v1(stop_words, ln_supported, lang_model, lang_utils, sent, perc_to_remove=0.1):\\n    embedds = []\\n    lang = lang_model.predict([sent])[0][0][0][-2:]\\n    if lang not in ln_supported:\\n        return np.zeros((600))\\n    word2id = lang_utils[lang][\"word2id\"]\\n    embedd = lang_utils[lang][\"embedd\"]\\n    words = sent.split()\\n    while len(words) and words[0] not in word2id:\\n        words = words[1:]\\n    if len(words) == 0:\\n        return np.zeros((600))\\n    ew = embedd[word2id[words[0]]]\\n    ew2 = None\\n    for i in range(1, len(words)):\\n        if words[i] not in stop_words[lang]:\\n            try:\\n                ew2 = embedd[word2id[words[i]]]\\n                embedds.append(np.concatenate([ew, ew2]))\\n                embedds.append(np.concatenate([ew2, ew]))\\n                ew = ew2\\n            except:\\n                None\\n    \\n    ew = embedd[word2id[words[0]]]\\n    if len(embedds) == 0:\\n        embedds.append(np.concatenate([ew, ew]))\\n    centroid = np.sum(embedds, axis=0) / len(embedds)\\n    return centroid\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "'''\n",
    "def vectorize_v1(stop_words, ln_supported, lang_model, lang_utils, sent, perc_to_remove=0.1):\n",
    "    embedds = []\n",
    "    lang = lang_model.predict([sent])[0][0][0][-2:]\n",
    "    if lang not in ln_supported:\n",
    "        return np.zeros((600))\n",
    "    word2id = lang_utils[lang][\"word2id\"]\n",
    "    embedd = lang_utils[lang][\"embedd\"]\n",
    "    words = sent.split()\n",
    "    while len(words) and words[0] not in word2id:\n",
    "        words = words[1:]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros((600))\n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    ew2 = None\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] not in stop_words[lang]:\n",
    "            try:\n",
    "                ew2 = embedd[word2id[words[i]]]\n",
    "                embedds.append(np.concatenate([ew, ew2]))\n",
    "                embedds.append(np.concatenate([ew2, ew]))\n",
    "                ew = ew2\n",
    "            except:\n",
    "                None\n",
    "    \n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    if len(embedds) == 0:\n",
    "        embedds.append(np.concatenate([ew, ew]))\n",
    "    centroid = np.sum(embedds, axis=0) / len(embedds)\n",
    "    return centroid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n# add outlier removal\\ndef vectorize_v2(stop_words, ln_supported, lang_model, lang_utils, sent, perc_to_remove=0.1):\\n    embedds = []\\n    lang = lang_model.predict([sent])[0][0][0][-2:]\\n    if lang not in ln_supported:\\n        return np.zeros((600))\\n    word2id = lang_utils[lang][\"word2id\"]\\n    embedd = lang_utils[lang][\"embedd\"]\\n    words = sent.split()\\n    while len(words) and words[0] not in word2id:\\n        words = words[1:]\\n    if len(words) == 0:\\n        return np.zeros((600))\\n    ew = embedd[word2id[words[0]]]\\n    ew2 = None\\n    for i in range(1, len(words)):\\n        if words[i] not in stop_words[lang]:\\n            try:\\n                ew2 = embedd[word2id[words[i]]]\\n                embedds.append(np.concatenate([ew, ew2]))\\n                embedds.append(np.concatenate([ew2, ew]))\\n                ew = ew2\\n            except:\\n                None\\n    \\n    ew = embedd[word2id[words[0]]]\\n    if len(embedds) == 0:\\n        embedds.append(np.concatenate([ew, ew]))\\n    centroid = np.sum(embedds, axis=0) / len(embedds) \\n    if len(embedds) * perc_to_remove >= 1:\\n        embedds = np.array(embedds)\\n        embedds = sorted(embedds.tolist(), key=lambda x: -np.array(dist([x], [centroid]))[0][0])\\n        centroid = np.sum(embedds[:-int(len(embedds) * perc_to_remove)], axis=0) / len(embedds) \\n    return centroid\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "'''\n",
    "# add outlier removal\n",
    "def vectorize_v2(stop_words, ln_supported, lang_model, lang_utils, sent, perc_to_remove=0.1):\n",
    "    embedds = []\n",
    "    lang = lang_model.predict([sent])[0][0][0][-2:]\n",
    "    if lang not in ln_supported:\n",
    "        return np.zeros((600))\n",
    "    word2id = lang_utils[lang][\"word2id\"]\n",
    "    embedd = lang_utils[lang][\"embedd\"]\n",
    "    words = sent.split()\n",
    "    while len(words) and words[0] not in word2id:\n",
    "        words = words[1:]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros((600))\n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    ew2 = None\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] not in stop_words[lang]:\n",
    "            try:\n",
    "                ew2 = embedd[word2id[words[i]]]\n",
    "                embedds.append(np.concatenate([ew, ew2]))\n",
    "                embedds.append(np.concatenate([ew2, ew]))\n",
    "                ew = ew2\n",
    "            except:\n",
    "                None\n",
    "    \n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    if len(embedds) == 0:\n",
    "        embedds.append(np.concatenate([ew, ew]))\n",
    "    centroid = np.sum(embedds, axis=0) / len(embedds) \n",
    "    if len(embedds) * perc_to_remove >= 1:\n",
    "        embedds = np.array(embedds)\n",
    "        embedds = sorted(embedds.tolist(), key=lambda x: -np.array(dist([x], [centroid]))[0][0])\n",
    "        centroid = np.sum(embedds[:-int(len(embedds) * perc_to_remove)], axis=0) / len(embedds) \n",
    "    return centroid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\na = vectorize_v2(stop_words, ln_supported, model, lang_utils, \"loose particles left inside aerospace components or equipment can cause catastrophic failure in aerospace industry  it is vital to identify the material type of these loose particles and eliminate them  this is a classification problem  and autoregressive (ar) model and learning vector quantization (lvq) networks have been used to classify loose particles inside components  more recently  the test objects have been changed from components to aerospace equipments  to improve classification accuracy  more data samples often have to be dealt with  the difficulty is that these data samples contain redundant information  and the aforementioned two conventional methods are unable to process redundant information  thus the classification accuracy is deteriorated  in this paper  the wavelet fisher discriminant is investigated for loose particle classifications  first  the fisher model is formulated as a least squares problem with linear in the parameters structure  then  the previously proposed two stage subset selection method is used to build a sparse wavelet fisher model in order to reduce redundant information  experimental results show the wavelet fisher classification method can perform better than ar model and lvq networks\")\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "'''\n",
    "a = vectorize_v2(stop_words, ln_supported, model, lang_utils, \"loose particles left inside aerospace components or equipment can cause catastrophic failure in aerospace industry  it is vital to identify the material type of these loose particles and eliminate them  this is a classification problem  and autoregressive (ar) model and learning vector quantization (lvq) networks have been used to classify loose particles inside components  more recently  the test objects have been changed from components to aerospace equipments  to improve classification accuracy  more data samples often have to be dealt with  the difficulty is that these data samples contain redundant information  and the aforementioned two conventional methods are unable to process redundant information  thus the classification accuracy is deteriorated  in this paper  the wavelet fisher discriminant is investigated for loose particle classifications  first  the fisher model is formulated as a least squares problem with linear in the parameters structure  then  the previously proposed two stage subset selection method is used to build a sparse wavelet fisher model in order to reduce redundant information  experimental results show the wavelet fisher classification method can perform better than ar model and lvq networks\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accelerate v2\n",
    "def vectorize_v3(stop_words, ln_supported, lang_utils, sent, perc_to_remove=0.1):\n",
    "    embedds = []\n",
    "    lang = \"fr\"\n",
    "    if lang not in ln_supported:\n",
    "        return np.full((600), -1e-28)\n",
    "    word2id = lang_utils[lang][\"word2id\"]\n",
    "    embedd = lang_utils[lang][\"embedd\"]\n",
    "    words = sent.split()\n",
    "    while len(words) and words[0] not in word2id:\n",
    "        words = words[1:]\n",
    "    if len(words) == 0:\n",
    "        return np.full((600), -1e-28)\n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    ew2 = None\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] not in stop_words[lang]:\n",
    "            try:\n",
    "                ew2 = embedd[word2id[words[i]]]\n",
    "                embedds.append(np.concatenate([ew, ew2]))\n",
    "                embedds.append(np.concatenate([ew2, ew]))\n",
    "                ew = ew2\n",
    "            except:\n",
    "                None\n",
    "    \n",
    "    ew = embedd[word2id[words[0]]]\n",
    "    if len(embedds) == 0:\n",
    "        embedds.append(np.concatenate([ew, ew]))\n",
    "    centroid = np.sum(embedds, axis=0) / len(embedds) \n",
    "    if len(embedds) * perc_to_remove >= 1:\n",
    "        embedds = np.array(embedds)\n",
    "        sims = np.squeeze(dist(embedds, [centroid]))\n",
    "        zipped_lists = np.array(sorted(zip(embedds, sims), key=lambda x:x[1]), dtype=np.object)\n",
    "        embedds = zipped_lists[:, 0]\n",
    "        centroid = np.sum(embedds[:-int(len(embedds) * perc_to_remove)], axis=0) / len(embedds) \n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a = vectorize_v3(stop_words, ln_supported, model, lang_utils, \"loose particles left inside aerospace components or equipment can cause catastrophic failure in aerospace industry  it is vital to identify the material type of these loose particles and eliminate them  this is a classification problem  and autoregressive (ar) model and learning vector quantization (lvq) networks have been used to classify loose particles inside components  more recently  the test objects have been changed from components to aerospace equipments  to improve classification accuracy  more data samples often have to be dealt with  the difficulty is that these data samples contain redundant information  and the aforementioned two conventional methods are unable to process redundant information  thus the classification accuracy is deteriorated  in this paper  the wavelet fisher discriminant is investigated for loose particle classifications  first  the fisher model is formulated as a least squares problem with linear in the parameters structure  then  the previously proposed two stage subset selection method is used to build a sparse wavelet fisher model in order to reduce redundant information  experimental results show the wavelet fisher classification method can perform better than ar model and lvq networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(stop_words, ln_supported, lang_utils, s1, s2, prt=True):\n",
    "    sim = dist([vectorize_v3(stop_words, ln_supported, lang_utils, s1)], [vectorize_v3(stop_words, ln_supported, lang_utils, s2)])[0]\n",
    "    if prt:\n",
    "        print(f\"similarity between \\\"{s1}\\\" and \\\"{s2}\\\" : {sim}\")\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "similarity between \"the purpose of this study is to develop a learning tool for high school student studying the scientific aspect of information and communication network\" and \"scientific\" : [0.72086544]\nsimilarity between \"introduction public health surveillance system need to be refined\" and \"500g\" : [0.00353887]\n"
     ]
    }
   ],
   "source": [
    "s2 = similarity(stop_words, ln_supported, lang_utils, \"the purpose of this study is to develop a learning tool for high school student studying the scientific aspect of information and communication network\", \"scientific\")\n",
    "s3 = similarity(stop_words, ln_supported, lang_utils, \"introduction public health surveillance system need to be refined\", \"500g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_generator(corpus):\n",
    "    for line in tqdm(corpus):\n",
    "        yield {\n",
    "            \"_index\" : 'mirion_data',\n",
    "            \"_source\" : {\n",
    "                \"title\": line[0],\n",
    "                \"data\" : line[1],\n",
    "                \"title_embeddings\": vectorize_v3(stop_words, ln_supported, lang_utils, line[0]),\n",
    "                \"data_embeddings\": vectorize_v3(stop_words, ln_supported, lang_utils, line[1])\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "source": [
    "before executing next cell execute in kibana :\n",
    "```\n",
    "\n",
    "DELETE /toy_data_docs_embb\n",
    "\n",
    "PUT /toy_data_docs_embb\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"knn\": true,\n",
    "      \"knn.space_type\": \"cosinesimil\"\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"title\": { \n",
    "        \"type\" : \"text\"\n",
    "      },\n",
    "      \"data\": { \n",
    "        \"type\" : \"text\"\n",
    "      },\n",
    "      \"title_embeddings\": {\n",
    "        \"type\": \"knn_vector\", \n",
    "        \"dimension\": 600\n",
    "      },\n",
    "      \"data_embeddings\": {\n",
    "        \"type\": \"knn_vector\", \n",
    "        \"dimension\": 600\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [00:18<00:00, 20.77it/s]\n",
      "/home/qfeuilla/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:208: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "out = helpers.bulk(es, doc_generator(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-25-0fd8eac69af3>:8: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if (query != np.full((600), 0)):\n",
      "minutes of meeting i = informa tion  d = decision  o = order  r = recommendation  1 / 10 mt_rms_mgpi hub_prm_meeting qualification and development projects_2016 03 18 projec t name: qualification projects (gim 203k  gim 206k) development projects ( sb 70   sb 71   bim 201 ) dat e : 17  0 3  201 6 type : technical and commercial m eeting with mirion lmn and muc minutes   no  : lmn   qp   2016   01 time: 1 0 :00 – 1 7 : 3 0 subject : te chnical discussions on qualification and development projects place : mirion lmn participants : mirion lmn : mr s   severine bruyere mr  sebastian buous mr  sylvain hoebanx mr  l udovic riquier mirion muc: mr  friedel mr  lautenbacher mr  dr  liebhart mr  dr  von loeben distribution list : all participants no   kat  top topic responsible date 01 i project schedule s ylvain inform ed about the up coming projects:   cap1400   > shida o wan npp   >first delivery 31 10 2017   > second delivery 31   12   2018   cap 1000   > lufeng 2 3   > sanmen 3   4 ap 10 0; \n",
      "\n",
      "\n",
      "mt_rms_mgpi hub_prm_meeting qualification and development projects_2016 03 18 pdf; \n",
      "\n",
      "\n",
      "        score 15.455289\n",
      "\n",
      "mirion technologies (mgpi) s a s société par actions simplifiée au capital de 22 025 010 euros siège social : 174  route d’eyguières   fr   13113 lamanon rcs : tarascon b 303 375 406  siret : 303 375 406 00020 n° tva intracommunautaire : fr 62 303 375 406 radiation monitoring systems division health physics division 174  route d’eyguières fr   13113 lamanon tel  +33(0)4 90 59 59 59 fax +33(0)4 90 59 55 18 www  mirion  com shanghai flying m e equipment room 801  no 88  jiang chang san road  shanghai 200436 r p of china  for the attention of mr  zhang  dear mr  zhang   further to your request   we are pleased to provide you with our best technical proposal i n relation with the following project : “ rms equipment for the spent fuel post   treatment industrial demonstration plant ” mirion technologies (mgpis) sas is the first supplier for rms applications especially for aerosols in the manufacturing factories and french post   fuel treatment sites  thanks to our long experience and all di; \n",
      "\n",
      "\n",
      "opp037836   shanghai flying   rms post process plant 200 t pdf; \n",
      "\n",
      "\n",
      "        score 16.335278\n",
      "\n",
      "mirion technologies (mgpi) s a s société par actions simplifiée au capital de 22 025 0 10 euros siège social : 174  route d’eyguières   fr   13113 lamanon rcs : tarascon b 303 375 406  siret : 303 375 406 00020 n° tva intracommunautaire : fr 62 303 375 406 radiation monitoring systems division health physics division 174  route d’eyguières fr   13113 lamanon tel  +33(0)4 90 59 59 59 fax +33(0)4 90 59 55 18 www  mirion  com shanghai flying m e equipment room 801  no 88  jiang chang san road  shanghai 200436 r p of china  for the attention of mr  zhang  dear mr  zhang   further to your request   and sending of technical offer opp037836  dated july 15  2019  we are pleased to provide you with our best price estimation i n relation with the following project : “ rms equipment for the spent fuel post   treatment industrial demonstration plant ” this offer was prepared on the basis of the limited information available to mirion technologies at the time of the offer (as tender documents have ; \n",
      "\n",
      "\n",
      "opp037836   shanghai flying   rms post process plant 200 t   commercial pdf; \n",
      "\n",
      "\n",
      "        score 16.498579\n",
      "\n",
      "mirion technologies (mgpi) s a s société par actions simplifiée au capital de 22 025 0 10 euros siège social : 174  route d’eyguières   fr   13113 lamanon rcs : tarascon b 303 375 406  siret : 303 375 406 00020 n° tva intracommunautaire : fr 62 303 375 406 radiation monitoring systems division health physics division 174  route d’eyguières fr   13113 lamanon tel  +33(0)4 90 59 59 59 fax +33(0)4 90 59 55 18 www  mirion  com shanghai flying m e equipment room 801  no 88  jiang chang san road  shanghai 200436 r p of china  for the attention of mr  zhang  dear mr  zhang   further to your request   and sending of technical offer opp037836  dated july 15  2019  we are pleased to provide you with our best price estimation i n relation with the following project : “ rms equipment for the spent fuel post   treatment industrial demonstration plant ” this offer was prepared on the basis of the limited information available to mirion technologies at the time of the offer (as tender documents have ; \n",
      "\n",
      "\n",
      "opp037836   shanghai flying   rms post process plant 200 t   commercial signed pdf; \n",
      "\n",
      "\n",
      "        score 16.498579\n",
      "\n",
      "mirion technologies (mgpi) s a s société par actions simplifiée au capital de 22 025 0 10 euros siège social : 174  route d’eyguières   fr   13113 lamanon rcs : tarascon b 303 375 406  siret : 303 375 406 00020 n° tva intracommunautaire : fr 62 303 375 406 radiation monitoring systems division health physics division 174  route d’eyguières fr   13113 lamanon tel  +33(0)4 90 59 59 59 fax +33(0)4 90 59 55 18 www  mirion  com shanghai flying m e equipment room 801  no 88  jiang chang san road  shanghai 200436 r p of china  for the attention of mr  zhang  dear mr  zhang   further to your request   and sending of technical offer opp037836  dated july 15  2019  we are pleased to provide you with our best price estimation i n relation with the following project : “ rms equipment for the spent fuel post   treatment industrial demonstration plant ” this offer was prepared on the basis of the limited information available to mirion technologies at the time of the offer (as tender documents have ; \n",
      "\n",
      "\n",
      "opp037836   shanghai flying   rms post process plant 200 t   commercial pdf; \n",
      "\n",
      "\n",
      "        score 16.509491\n",
      "\n",
      "page : 2/10 ce document est la propriété de mirion technologies (mgpi) sa et ne peut être reproduit ou communiqué sans autorisation écrite this document contains proprietary information of mirion technologies (mgpi)  sa  and may not be used  disclosed  copied or red istributed except as specifically authorized in writing by mirion 153116 a n° 113608 c revisions indice/date/by modified pages description of the modification page : 3/10 ce document est la propriété de mirion technologies (mgpi) sa et ne peut être reproduit ou communiqué sans autorisation écrite this document contains proprietary information of mirion technologies (mgpi)  sa  and may not be used  disclosed  copied or red istributed except as specifically authorized in writing by mirion 153116 a n° 113608 c table of content 1 introduction     4 2 reference documents     4 3 general requirements     4 3 1 l oading / unloading     4 3 2 t ests     4 3 3 t est conditions     5 3 3 1 general test conditions     5 3 3 2 ngm 216 ; \n",
      "\n",
      "\n",
      "00153116_a nuclear test specification ngm216 pdf; \n",
      "\n",
      "\n",
      "        score 19.54816\n",
      "\n",
      "page : 2/15 ce document est la propriété de mirion technologies (mgpi) sa et ne peut être reproduit ou communiqué sans autorisation écrite this document contains proprietary information of mirion technologies (mgpi)  sa  and may not be used  disclosed  copied or red istributed except as specifically authorized in writing by mirion 149500 b n° 113608 c revisions indice/date/by modified pages description of the modification b/may2011/cac p6 p7 p8 all 1  add §3 5 qa requirements to apply 10cfr50 appendix b and 10cfr part 21 in th is program of test 2  add §3 6 agreement for additional sources 3  §4 1 : suppression of probe 3 in test program 4  english vocabulary page : 3/15 ce document est la propriété de mirion technologies (mgpi) sa et ne peut être reproduit ou communiqué sans autorisation écrite this document contains proprietary information of mirion technologies (mgpi)  sa  and may not be used  disclosed  copied or red istributed except as specifically authorized in writing by mirion; \n",
      "\n",
      "\n",
      "00149500_b nuclear test specification pdf; \n",
      "\n",
      "\n",
      "        score 19.874922\n",
      "\n",
      "mirion technologies (mgpi) s a  radiation monitoring systems division health physics division route d’eyguières fr   13113 lamanon tel  +33(0)4 90 59 59 59 fax +33(0)4 90 59 55 18 www mirion com soc iété anonyme au capital de 22 025 010 euros usine et siège social : route d’eyguières  lieu   dit calès fr   13113 lamanon rcs : tarascon b 303 375 406  siret : 303 375 406 00020 n° tva intracommunautaire : fr 62 303 375 406 james fisher nuclear ltd  gordon house  sceptre way  preston  pr5 6aw united kingdom to the attention of m rs jenny stewart dear mrs stewart   further to your invitation t o t ender  we are pleased to send you our updated technic al offer related to the supply of: “ one checkpoint: body tm rtm 860   uk2 “ we stay at your entire disposal for any further information you may require  yours faithfully  harald struwe sales manager date : 10 / 04/2017 v ref /y ref  : référence client n ref /o ref  : opp031381 tel  : +49 40 85 19 31 60 objet/subj  : technical and commercial of; \n",
      "\n",
      "\n",
      "opp031381   rtm860uk2 for james fisher nuclear pdf; \n",
      "\n",
      "\n",
      "        score 20.388083\n",
      "\n",
      "nuclear island (n i) material crate marking project consignee company: liaoning hongyanhe nuclear power co ltd  address: ba building hongyanhe nuclear power station  donggang town  wafangdian   dalin city  liaoning province people’s republic of china post code: 116319 aa liaison: chen jun telephone: 15998601792 fax: 0411  82346111 e mail: chenjun cgnpc com cn shipper company  address: mirion technologies  13113 lamanon france post code: 13113 material description ： kit for ping206s modification ref nom002253 pis tools ref n om002010 s/n 16003044 16003045  16003046  16003047 ni lot 104 b contract no  ： gnesaa00240 consignee ： lhnp purchase order ： gnesaa00240 /vo   010 storage level: 1 (liaoning hongyanhe nuclear power co   ltd ) cleanliness classes: b crate no  ： aa104b0 5 5 8 gross wt  ： 38 kg net wt  ： 20 kg s izes ： 174 x 64 x 65 cm unit ： set consignee code ： lhnp; \n",
      "\n",
      "\n",
      "nuclear island hyh vo010 pdf; \n",
      "\n",
      "\n",
      "        score 20.481033\n",
      "\n",
      "rapport d’essais  test report rapport d’essais de type  type test report page 2/2 ce document est la propriété de mirion technologies (mgpi) sa et ne peut être reproduit ou communiqué sans autorisation écrite 155272 a this document contains proprietary information of mirion technologies (mgpi)  sa  and may not be used  disclosed  copied or red istributed except as specifically authorized in writing by mirion format 27 221 j; \n",
      "\n",
      "\n",
      "00155272_a_ret nuclear pm205 pdf; \n",
      "\n",
      "\n",
      "        score 22.289223\n",
      "/home/qfeuilla/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:208: ElasticsearchWarning: this request accesses system indices: [.kibana_1, .kibana_92668751_admin_1], but in a future major version, direct access to system indices will be prevented by default\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "# hybrid v3\n",
    "size = 5\n",
    "\n",
    "query = clean_text(\"nuclear\")\n",
    "query_vec = vectorize_v3(stop_words, ln_supported, lang_utils, query)\n",
    "\n",
    "emb_weight = 8\n",
    "if (query != np.full((600), 0)):\n",
    "  query_body = {\n",
    "    \"min_score\" : emb_weight + 1.1,\n",
    "    \"query\": {\n",
    "      \"function_score\": {\n",
    "        \"query\": {\n",
    "          \"bool\": { \n",
    "            \"should\" : [\n",
    "              {\n",
    "                \"multi_match\" : { \n",
    "                  \"query\": query,\n",
    "                  \"fields\": [\"data\", \"title\"]\n",
    "                }\n",
    "              },\n",
    "              {\n",
    "                \"match_all\": { }\n",
    "              }\n",
    "            ],\n",
    "            \"minimum_should_match\" : 0\n",
    "          }\n",
    "        },\n",
    "        \"functions\": [\n",
    "        {\n",
    "          \"script_score\" : {\n",
    "            \"script\" : {\n",
    "              \"source\": \"cosineSimilarity(params.query_value, doc[params.field1]) + cosineSimilarity(params.query_value, doc[params.field2]) * 2\",\n",
    "              \"params\": {\n",
    "                \"field1\": \"title_embeddings\",\n",
    "                \"field2\": \"data_embeddings\",\n",
    "                \"query_value\": query_vec\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"weight\": emb_weight\n",
    "        }\n",
    "        ],\n",
    "        \"score_mode\": \"sum\",\n",
    "        \"boost_mode\": \"sum\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "else:\n",
    "  query_body = {\n",
    "    \"query\": {\n",
    "      \"multi_match\": { \n",
    "        \"query\": query,\n",
    "        \"fields\": [\"data\", \"title\"]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "res = es.search(index=\"mirion_data\", body=query_body)\n",
    "print('\\n\\n'.join(['; \\n\\n\\n'.join([res['hits']['hits'][i]['_source']['title'][:1000], res['hits']['hits'][i]['_source']['data'], f\"        score {res['hits']['hits'][i]['_score']}\"]) for i in range(len(res['hits']['hits']))][::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['took'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "« ce document contient des informations sensibles rel evant du secret et juridiquement protégées  il est réservé à l’usage exclusif des personnes désignées comme destinataires du document et/ou aut orisées à y accéder  il est illégal de photocopier  distribuer  divulguer ou d’utiliser de toute autre manière les informations contenues dans ce document sans accord du service émetteur de la din »  di; \n\n\n1_emeic101986b   cctp krt court terme pdf; \n\n\n        score 1.6808186\n\n1 division radiation monitoring systems catalogue produits 1 2 download the catalog learn more about the cpo smart monitor and other members of the mirion smart monitoring family: catalogue produits moniteurs d’irradiation 5 moniteurs aérosols 23 moniteurs iode 43 moniteurs gaz rares 51 moniteurs mixtes 63 moniteurs liquides 71 moniteurs d’analyse spectrale 75 échantillonneurs 83 moniteurs de mesu; \n\n\n1 rms catalogue pdf; \n\n\n        score 1.1927977\n"
     ]
    }
   ],
   "source": [
    "# classic match to compare\n",
    "query = \"étanchéité\"\n",
    "query_vec = vectorize_v3(stop_words, ln_supported, lang_utils, clean_text(query))\n",
    "\n",
    "query_body = {\n",
    "    \"size\": 8,\n",
    "    \"query\": {\n",
    "      \"multi_match\": { \n",
    "            \"query\": query,\n",
    "            \"fields\": [\"data\", \"title\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "res = es.search(index=\"mirion_data\", body=query_body)\n",
    "print('\\n\\n'.join(['; \\n\\n\\n'.join([res['hits']['hits'][i]['_source']['title'][:400], res['hits']['hits'][i]['_source']['data'], f\"        score {res['hits']['hits'][i]['_score']}\"]) for i in range(len(res['hits']['hits']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}